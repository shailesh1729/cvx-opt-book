(sec:prob:intro)=
# Introduction

Our notes will be based on the axiomatic treatment of probability.

```{index} Random experiment
```
```{prf:definition} Random experiment
:label: def-prob-random-experiment

A *random experiment* is an experiment in which the
outcomes are nondeterministic.
In other words, different outcomes can occur each time
the experiment is run.
```

## Sample Space

```{index} Sample space
```
```{prf:definition} Sample space
:label: def-prob-sample-space

The *sample space* associated with a random experiment
is the set of all possible outcomes of the experiment.
```

We shall often denote the sample space by the symbol
$\Omega$. Individual outcomes shall often be denoted
by $\zeta$.

### Sigma Fields

```{index} Field; probability
```
```{prf:definition} Field
:label: def-prob-field

Consider the sample space $\Omega$
and a certain collection of subsets of $\Omega$ denoted by $\FFF$.
Let $E$ and $F$ be arbitrary members of $\FFF$.

We say that $\FFF$ forms a *field* if meets the following
constraints:

1. $\EmptySet \in \FFF$.
1. $\Omega \in \FFF$.
1. If $E, F \in \FFF$ then $E \cup F \in \FFF$ and $E \cap F \in \FFF$.
1. If $E \in \FFF$ then $\Omega \setminus E = E^c \in \FFF$.

In other words,
$\FFF$ contains the empty set and the sample space,
$\FFF$ is closed under finite union and intersection,
and $\FFF$ is closed under complement.
```

```{note}
The notion of field here is different from the notion
of algebraic fields (e.g. $\RR$ and $\CC$).
```

Often, we need to work with infinite set of outcomes
and events. We have to deal with situations which
involve countable unions and intersections of sets.
To handle such cases, we need to extend the
definition of field.

```{index} $\sigma$ field, $\sigma$ algebra
```
```{prf:definition} $\sigma$ field
:label: def-prob-sigma-field

Consider an infinite sample space $\Omega$
and a certain collection of subsets of $\Omega$ denoted by $\FFF$.
We say that $\FFF$ is a $\sigma$-*field* if
it is a field and it is closed
under countable unions, intersections and complements.
This is also known as a $\sigma$-*algebra*.
```


```{prf:definition} Field generated by a collection of sets
:label: res-prob-generated-field

Let $\{ A_i \}_{i \in I}$ be a collection of subsets of $\Omega$
where $I$ is an index set.
Let $\FFF$ be the smallest $\sigma$-field such that
$A_i \in \FFF$ for every $i \in I$.
Then $\FFF$ is called the $\sigma$-field generated
by $\{ A_i \}$.
```

1. Here by smallest, we mean that if there is any other $\sigma$-field
   $\GGG$ such that $A_i \in \GGG$ for every $i \in I$, then
   $\FFF \subseteq \GGG$.
1. It is possible that there is no $\sigma$-field $\FFF$ such that
   $A_i \in \FFF$ for every $i \in I$.
1. If the index set $I$ is finite, then the generated field doesn't
   require closure under countable union/intersection. Closure under
   finite union and intersection is sufficient.
1. It may not be possible to visualize every member of $\FFF$ easily
   from the descriptions of $A_i$. 

```{prf:example} Field generated by 2 sets
:label: ex-prob-gen-field-2-sets

Let $\Omega$ be an arbitrary sample space. Let $A$ and $B$
be two subsets of $\Omega$ which are not necessarily disjoint.
We shall construct the field $\FFF$ generated by $A$ and $B$.

1. Since $A \in \FFF$, hence $A^c \in \FFF$.
1. Similarly, $B^c \in \FFF$.
1. Since $A$ and $B$ and their complements are in $\FFF$,
   hence $AB, AB^c, A^c B, A^c B^c \in \FFF$
   as $\FFF$ is closed under intersection.
1. Let us name them $E = AB$, $F = AB^c$, $G = A^c B$ and $H = A^c B^c$.
1. We can see that these four sets $E, F, G, H$
   are disjoint and

   $$
   \Omega = E \cup F \cup G \cup H.
   $$
1. The empty set $\EmptySet$ doesn't contain any of these sets ${4 \choose 0}$.
1. There are ${4 \choose 1} = 4$ of these disjoint subsets. 
1. There are ${4 \choose 2} = 6$ pair-wise unions of these $4$ sets.
1. There are ${4 \choose 3} = 4$ unions of $3$ of these $4$ subsets.
1. There is ${4 \choose 4} = 1$ union of all the $4$ subsets which is $\Omega$.
1. A total of $1 + 4 + 6 + 4 + 1 = 16 = 2^4$ possible subsets are formed.
1. In particular, note that $A = E \cup F$, $B = E \cup G$,
   $A^c = G \cup H$ and $B^c  = F \cup H$.

We claim that this collection of $16$ subsets of $\Omega$ is
the field $\FFF$. To show this, we need to show that
this collection of $16$ sets is closed under union, intersection
and complement.

1. Unions of these $16$ sets are trivially in $\FFF$.
1. Since $\Omega = E \cup F \cup G \cup H$, hence complements
   of $E, F, G, H$ are the four triple unions of the remaining sets
   which are already included.
1. Similarly, since any set is composed of the union of a few of $E, F, G, H$, hence
   its complement is composed of the union of remaining sets.
   Hence, this collection is closed under complement.
1. Intersections of these $16$ events are also some unions of $E,F,G, H$
   Hence, the collection is closed under intersection also.
1. Hence this must be $\FFF$.
```

```{index} Borel field
```
```{prf:definition} Borel field
:label: def-prob-borel-field

The $\sigma$-field generated by the open and closed
intervals of the real line $\RR$ is known as the
*Borel field* of events on the real line.
```

There are other subsets of real line which are not
included in the Borel field. However, they don't 
happen to be of much engineering and scientific
interest. 

### Events

```{index} Event; probability
```
```{prf:definition} Event
:label: def-prob-event
An *event* is a subset of the sample space of
a random experiment that satisfies some given
constraints.

Let $\Omega$ be the sample space of a random experiment.
Let $\FFF$ be a $\sigma$-field of subsets of $\Omega$.
Then every member of $\FFF$ is an event.
```

```{note}
Events are the subsets of the
sample space to which probabilities can
be assigned. 
For finite sample spaces, any subset can
be an event. For infinite sample spaces,
it may not be possible to meaningfully
assign probabilities to every possible
subset. We need the notion of a $\sigma$-field
which is a collection of subsets of the
sample space satisfying closure under countable unions,
intersections and complements.
The subsets belonging to a $\sigma$-field
can be assigned probabilities and are
events.
```

```{index} Singleton event, Elementary event
```
```{prf:definition} Elementary event
:label: def-prob-elementary-event

An event consisting of only one outcome
is called a *singleton event* or an *elementary event*.
```

```{index} Certain event
```
```{prf:definition} Certain event
:label: def-prob-certain-event

The sample space $\Omega$ is known as the *certain event*.
```
Since $\Omega$ contains all possible outcomes of an
experiment, hence this event always occurs whenever
the experiment is run.

```{index} Null event
```
```{prf:definition} Null event
:label: def-prob-null-event

The empty set $\EmptySet$ is known as the *null event*.
```

The null event never occurs.

```{index} Mutually exclusive events
```
```{prf:definition}  Mutually exclusive events
:label: def-prob-mutually-exclusive-events

Let $E$ and $F$ be two events. If $E$ and $F$ are disjoint sets
then we say that the two events are mutually exclusive.
```

## Probability Measure

We next provide an axiomatic definition of a probability measure.

```{index} Probability measure
```
```{prf:definition} Probability measure
:label: def-prob-probability-measure


Let $\Omega$ be a sample space and
let $\FFF$ be a $\sigma$ field of subsets of $\Omega$.
A *probability measure* is a set function $\PP : \FFF \to \RR$
that assigns to every event $E \in \FFF$ a real number
$\PP(E)$ called the probability of the event $E$ satisfying
the following rules:

1. Nonnegativity: $\PP(E) \geq 0$.
1. Unit measure: $\PP(\Omega) = 1$.
1. Additivity: $\PP(E \cup F) = \PP(E) + \PP(F)$ if $E F = \EmptySet$.
```

We can write it in the form of axioms
(first introduced by Andrey Kolmogorov).

```{prf:axiom} First axiom: nonnegativity
:label: ax-prob-nonnegativity

The probability of an event is a nonnegative real number.

$$
\PP(E) \in \RR, \PP(E) \geq 0 \quad \Forall E \in \FFF.
$$
```
This axiom implies that the probability is always finite.

```{prf:axiom} Second axiom: unit measure
:label: ax-prob-unit-measure

$$
\PP(\Omega) = 1.
$$
```

```{prf:axiom} Third axiom: additivity
:label: ax-prob-additivity

Let $E, F \in \FFF$ be two disjoint sets. Then

$$
\PP(E \cup F) = \PP(E) + \PP(F).
$$
```

```{index} Probability space
```
```{prf:definition} Probability space
:label: def-prob-probability-space

A sample space endowed with a $\sigma$-field and
a probability measure is known as a *probability space*.
In other words,
let $\Omega$ be the sample space of a random experiment,
let $\FFF$ be a $\sigma$ field of subsets of $\Omega$
and let $\PP$ be a probability measure defined on $\FFF$.
Then the triplet $(\Omega, \FFF, \PP)$ is known as
a probability space.
```


We next establish some basic facts about a probability measure.



### Basic Properties

```{prf:theorem} Properties of a probability measure
:label: res-prob-prob-measure-props

Let $(\Omega, \FFF, \PP)$ be a probability space.
Then for the events contained in $\FFF$ satisfy
the following properties. 

1. Probability of null event: $\PP(\EmptySet) = 0$.
1. $\PP(E F^c) = \PP(E) - \PP(EF)$.
1. Complement rule: $\PP(E) = 1 - \PP(E^c)$.
1. Sum rule: $\PP(E \cup F) = \PP(E) + \PP(F) - \PP(EF)$.
1. Monotonicity: If $E \subseteq F$, then 

   $$
   \PP (E)\leq \PP(F).
   $$
1. Numeric bound: 

   $$
   0 \leq \PP(E) \leq 1.
   $$
1. Finite additivity: For any positive integer $n$, we have

   $$
   \PP\left ( \bigcup_{i=1}^n E_i \right) = \sum_{i=1}^n \PP(E_i)
   $$
   if $E_1, E_2, \dots, E_n$ are pairwise disjoint events.
```

```{prf:proof}
(1)

1. $\EmptySet$ and $\Omega$ are disjoint.
1. Hence 

   $$
   \PP(\EmptySet \cup \Omega) = \PP(\EmptySet) + \PP(\Omega).
   $$
1. This simplifies to

   $$
   1 = \PP(\EmptySet) + 1.
   $$
1. Hence $\PP(\EmptySet) = 0$.

(2) 

1. Recall that $E F^c$ and $EF$ are disjoint sets
   with $E F^c \cup EF = E$.
1. Hence

   $$
   \PP(E) = \PP(E F^c) + \PP(E F).
   $$
1. Hence

   $$
   \PP(E F^c) = \PP(E) - \PP(EF).
   $$

(3)
1. $E$ and $E^c$ are disjoint with $E \cup E^c = \Omega$.
1. Hence

   $$
   1 = \PP(\Omega) = \PP(E \cup E^c) = \PP(E) + \PP(E^c).
   $$
1. Hence $\PP(E) = 1 - \PP(E^c)$.

(4)

1. Recall that we can split $E \cup F$ into disjoint sets

   $$
   E \cup F = EF^c \cup EF \cup E^c F.
   $$
1. By additivity, we have

   $$
   \PP(E \cup F) &= \PP(EF^c \cup (EF \cup E^c F))\\
   &= \PP(EF^c) + \PP(EF \cup E^c F)\\
   &= \PP(EF^c) + \PP(EF) + \PP(E^c F)\\
   &= \PP(E) - \PP(EF) + \PP(EF) + \PP(F) - \PP(EF)\\
   &= \PP(E) + \PP(F) - \PP(EF).
   $$

(5)
1. We have $F = F E \cup F E^c = E \cup F E^c$.
1. $E$ and $FE^c$ are disjoint.
1. Then
   
   $$
   \PP (F) = \PP(E \cup F E^c) = \PP(E) + \PP(F E^c). 
   $$
1. By nonnegativity, $\PP(F E^c) \geq 0$.
1. Hence $\PP(E) \leq \PP(F)$.

(6)

1. We have $\PP(E) = 1 - \PP(E^c)$.
1. But $\PP(E^c) \geq 0$.
1. Hence $\PP(E) \leq 1$.

(7)

1. The statement is trivially true for $n=1$.
1. The statement is true for $n=2$ by the additivity rule.
1. Assume that the statement is true for some $k \geq 2$.
1. In other words, for every collection of events
   $E_1, \dots, E_k$, such that the events are pairwise
   disjoint, we have

   $$
   \PP\left ( \bigcup_{i=1}^k E_i \right) = \sum_{i=1}^k \PP(E_i).
   $$
1. Let $E_1, E_2, \dots, E_k, E_{k+1}$ be a collection of $k+1$
   pairwise disjoint events. Define $E= \bigcup_{i=1}^k E_i$.
1. We have $E \cap E_{k+1} = \EmptySet$. 
1. Then

   $$
   \PP\left ( \bigcup_{i=1}^{k+1} E_i \right)
   &= \PP ( E \cup E_{k + 1}) \\
   &= \PP (E) + \PP(E_{k + 1})\\
   &= \PP\left ( \bigcup_{i=1}^k E_i \right) + \PP(E_{k + 1})\\
   &= \sum_{i=1}^k \PP(E_i) + \PP(E_{k + 1})\\
   &= \sum_{i=1}^{k+1} \PP(E_i).
   $$
1. By principle of mathematical induction, the statement is true
   for every $n$.
```


```{note}
We assign probabilities to events and not to individual outcomes
of a random experiment.
If the sample space is finite or countable, often it is convenient
to assign probabilities to individual outcomes. One should treat
this as assignment of probability
to the event consisting of a single outcome;
a singleton event.
```

In the following, we shall assume that a
probability space $(\Omega, \FFF, \PP)$ has
been given and all events are contained in $\FFF$.


```{prf:theorem} Union of three events
:label: res-prob-union-3-events

Let $A, B, C$ be three events. Then

$$
\PP(A \cup B \cup C) = \PP(A) + \PP(B) + \PP(C) - \PP(AB) - \PP(BC) - \PP(AC) + \PP(ABC).
$$
```
```{prf:proof}
Define $D = B \cup C$.

1. Then 

   $$
   \PP(A \cup B \cup C) = \PP(A \cup D) = \PP(A) + \PP(D) - \PP(AD).
   $$
1. Further

   $$
   \PP(D) = \PP(B \cup C) = \PP(B) + \PP(C) - \PP(BC).
   $$
1. Note that $AD = AB \cup AC$.
1. Also $AB \cap AC = ABC$.
1. Hence

   $$
   \PP(AD) = \PP(AB \cup AC) = \PP(AB) + \PP(AC) - \PP(ABC).
   $$
1. Putting these back, we get

   $$
   \PP(A \cup B \cup C) 
   &= \PP(A) + (\PP(B) + \PP(C) - \PP(BC)) - (\PP(AB) + \PP(AC) - \PP(ABC)) \\
   &= \PP(A) + \PP(B) + \PP(C) - \PP(AB) - \PP(BC) - \PP(AC) + \PP(ABC).
   $$
```

### Inclusion-Exclusion Principle

{prf:ref}`res-prob-union-3-events` can be extended
to the union of $n$ events. This is known
as the inclusion-exclusion principle.

```{index} Inclusion-exclusion principle
```
```{prf:theorem} Inclusion-exclusion principle
:label: res-prob-inc-ex

Let $A_1, A_2, \dots, A_n$ be $n$ events
in a probability space $(\Omega, \FFF, \PP)$.
Then

$$
\PP \left( \bigcup_{i=1}^n A_i \right )
&= S_1 - S_2 + S_3 - \dots + (-1)^{n + 1} S_n\\
&= \sum_{k=1}^n (-1)^{k+1}S_k
$$
where $S_k$ is the sum of the probability of all $k$-cardinality
intersections among the sets $A_1, \dots, A_n$.
In particular,

$$
& S_1 = \sum_{i=1}^n \PP(A_i) \\
& S_2 = \sum_{1 \leq i < j \leq n} \PP(A_i A_j) \\
& S_3 = \sum_{1 \leq i < j < k \leq n} \PP(A_i A_j A_k)\\
& \vdots\\
& S_n = \PP(A_1 A_2 \dots A_n).
$$
In general for every $k \in 1,\dots,n$, we can write:

$$
S_k = \sum_{1 \leq i_1 < i_2 <  \dots < i_k \leq n} \PP(A_{i_1} A_{i_2} \dots  A_{i_k}).
$$
```
It is known as inclusion-exclusion principle since $S_1$ is
included then $S_2$ is excluded, then $S_3$ is included and
so on.

```{prf:proof}
The proof is based on mathematical induction.
```


### Boole's Inequality

```{index} Boole's inequality, Union bound
```
````{prf:theorem} Boole's inequality
:label: res-prob-boole-inequality

Let  $A_1, A_2, \dots, A_n$ be a finite collection of events.
Then we have

$$
\PP \left ( \bigcup_{i=1}^n  A_i \right) \leq \sum_{i=1}^n \PP \left ( A_i \right).
$$
````
````{prf:proof}
We prove it using induction.

1. For $n=1$, obviously

    $$
    \PP (A_1) \leq \PP (A_1).
    $$
1. Assume the inequality is true for the set of $n$ events for some $n \geq 1$.
   In other words,

    $$
    \PP \left ( \bigcup_{i=1}^n  A_i \right) \leq \sum_{i=1}^n \PP \left ( A_i \right).
    $$
1. Since 

    $$
    \PP (A \cup B ) = \PP (A) + \PP(B) - \PP (A \cap B),
    $$
    hence

    $$
    \PP \left ( \bigcup_{i=1}^{n + 1}  A_i \right)  
    = \PP \left ( \bigcup_{i=1}^n  A_i \right) 
    + \PP (A_{n + 1}) - \PP \left ( \bigcup_{i=1}^n  A_i \bigcap A_{n +1} \right  ). 
    $$
1. Since

    $$
    \PP \left ( \bigcup_{i=1}^n  A_i \bigcap A_{n +1} \right  ) \geq 0,
    $$
    hence

    $$
    \PP \left ( \bigcup_{i=1}^{n + 1}  A_i \right) 
    \leq  \PP \left ( \bigcup_{i=1}^n  A_i \right) + \PP (A_{n + 1}) 
    \leq \sum_{i=1}^{n + 1} \PP \left ( A_i \right).
    $$
````

## Joint and Conditional Probability

### Joint Probability

```{index} Joint probability
```
```{prf:definition} Joint probability
:label: def-prob-joint-probability

Let $A$ and $B$ be two different events. Then
the *joint probability* of the events $A$
and $B$ is the probability that the two events
occur together and is given by $\PP(A B)$.

Similarly, let $\{A_i \}_{i \in I}$ be a collection
of events indexed by the set $I$. Then their *joint probability*
is given by

$$
\PP \left (\bigcap_{i \in I} A_i \right ).
$$
In other words, it is the probability of every event
happening together.
```


### Conditional Probability

```{index} Conditional probability
```
```{prf:definition} Conditional probability
:label: def-prob-conditional-probability

Let $A$ and $B$ be two events.
Assume that $\PP(A) > 0$.
The *conditional probability* of the event $B$
given that the event $A$ has happened is denoted by
$\PP(B | A)$. It is defined as

$$
\PP(B | A) \triangleq \frac{\PP(AB)}{\PP(A)}
\text{ if } \PP(A) > 0.
$$
```

By definition, we can see that

$$
\PP(A B) = \PP(B | A) \PP(A) = \PP(A | B) \PP(B).
$$


### Independence

```{index} Independence
```
```{prf:definition} Independence of two events
:label: def-prob-independence-2

Let $A$ and $B$ be two events
with $\PP(A) > 0$ and $\PP(B) > 0$.
We say that $A$ and $B$ are *independent* if and only if

$$
\PP(A B) = \PP(A) \PP(B).
$$
```

It follows that for independent events

$$
\PP(B | A) = \PP(B)
\text{ and }
\PP(A | B) = \PP(A).
$$

```{prf:definition} Independence of three events
:label: def-prob-independence-3

Let $A, B$ and $C$ be three events
with nonzero probabilities.
We say that $A, B$ and $C$ are *jointly independent* if and only if

$$
& \PP(A B C) = \PP(A) \PP(B) \PP(C)\\
& \PP(A B) = \PP(A) \PP(B)\\
& \PP(B C) = \PP(B) \PP(C)\\
& \PP(A C) = \PP(A) \PP(C).
$$
```


```{prf:definition} Independence of $n$ events
:label: def-prob-independence-n

Let $A_1, A_2, \dots, A_n$ be $n$ events contained in $\FFF$.
We say that $A_1, A_2, \dots, A_n$ are *jointly independent*
if and only if

$$
& \PP(A_{i_1} A_{i_2}) = \PP(A_{i_1}) \PP(A_{i_2})\\ 
& \PP(A_{i_1} A_{i_2} A_{i_3}) = \PP(A_{i_1}) \PP(A_{i_2}) \PP(A_{i_3})\\ 
& \vdots\\
& \PP(A_{i_1} A_{i_2} A_{i_3} \dots A_{i_k}) 
= \PP(A_{i_1}) \PP(A_{i_2}) \PP(A_{i_3}) \dots \PP(A_{i_k})\\ 
& \vdots\\
& \PP(A_1 A_2 \dots A_n) = \PP(A_1) \PP(A_2) \dots \PP(A_n)
$$
for all combinations of indices such that $1 \leq i_1 < i_2 < \dots < i_k \leq n$.
```

## Compound Experiments

Often we need to examine the outcomes of different experiments together.
Here are some examples:

- Tossing a coin and throwing a dice
- Tossing a coin twice in succession (first and second tosses are separate experiments)

Two or more experiments together form a compound experiment.
Repeated trials are an example of a compound experiment.

```{index} Compound experiment
```
```{prf:definition} Compound experiment
:label: def-prob-compound-experiment

Let $A$ and $B$ be two different experiments.
Let $\Omega_1$ be the sample space of $A$
and $\Omega_2$ be the sample space of $B$.
Then the sample space of the *compound experiment*
is given by the Cartesian product $\Omega_1 \times \Omega_2$.
Let $E_1$ be an event associated with experiment $A$
and $E_2$ be an event associated with experiment $B$.
Then $E = E_1 \times E_2$ is a *compound event* associated with
the compound experiment.

$$
E_1 \times E_2 = \{\zeta = (\zeta_1, \zeta_2) \ST \zeta_1 \in E_1, \zeta_2 \in E_2 \}.
$$
```

```{index} Independent experiments
```
```{prf:definition} Independent experiments
:label: def-prob-independent-experiment

Two experiments are called *independent*
if the outcome of one experiment doesn't
depend on the (past, present or future) outcomes
of the other experiment.
In that case, for every product event $E = E_1 \times E_2$, 
we can write

$$
\PP(E) = \PP(E_1) \PP(E_2).
$$
```

```{prf:example} Repeated coin tosses
:label: ex-prob-repeated-coin-tosses

Consider tossing a coin $n$ times.
1. Assume that the each toss is an independent random experiment.
1. The outcomes of each experiment are $H$ (head) and $T$ (tail).
1. Let $\PP(H) = p$ and $\PP(T) = q = 1 - p$.
1. The outcome of $n$ tosses can be described as a string
   of $n$ letters each of which is $H$ or $T$.
1. There are $2^n$ possible strings. They form the sample space
   of the compound experiment.
1. Let a particular string have $k$ heads and $n-k$ tails. 
1. Then the probability of this string is given by

    $$
    \PP(\zeta_1, \dots, \zeta_n) = \prod_{i=1}^n \PP(\{ \zeta_i \})
    = p^k q^{n - k}.
    $$
1. There are $n \choose k$ strings which consist of $k$ heads and
   $n-k$ tails.
1. Each of these strings (singleton events) are mutually exclusive
   of each other.
1. Hence, by the additivity axiom, the probability of having $k$
   heads and $n-k$ tails in $n$ trials is given by

   $$
   {n \choose k} p^k q^{n - k}.
   $$
```


## Countable Additivity

Often, we need to work with problems where we need to estimate
the probability of a countable union of events. The basic
axioms of a probability measure are unable to handle
this.


```{prf:axiom} Fourth axiom: countable additivity
:label: ax-prob-countable-additivity

Let $E_1, E_2, \dots$ be a (countable) sequence of mutually exclusive
events (disjoint sets). Then

$$
\PP\left ( \bigcup_{i=1}^{\infty} E_i \right) = \sum_{i=1}^{\infty} \PP(E_i).
$$
```

