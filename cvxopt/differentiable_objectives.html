
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>9.5. Constrained Optimization I &#8212; Topics in Signal Processing</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script kind="utterances">

    var commentsRunWhenDOMLoaded = cb => {
    if (document.readyState != 'loading') {
        cb()
    } else if (document.addEventListener) {
        document.addEventListener('DOMContentLoaded', cb)
    } else {
        document.attachEvent('onreadystatechange', function() {
        if (document.readyState == 'complete') cb()
        })
    }
}

var addUtterances = () => {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src = "https://utteranc.es/client.js";
    script.async = "async";

    script.setAttribute("repo", "shailesh1729/tisp");
    script.setAttribute("issue-term", "pathname");
    script.setAttribute("theme", "github-light");
    script.setAttribute("label", "ðŸ’¬ comment");
    script.setAttribute("crossorigin", "anonymous");

    sections = document.querySelectorAll("div.section");
    if (sections !== null) {
        section = sections[sections.length-1];
        section.appendChild(script);
    }
}
commentsRunWhenDOMLoaded(addUtterances);
</script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"AA": "\\mathbb{A}", "BB": "\\mathbb{B}", "CC": "\\mathbb{C}", "DD": "\\mathbb{D}", "EE": "\\mathbb{E}", "FF": "\\mathbb{F}", "GG": "\\mathbb{G}", "HH": "\\mathbb{H}", "II": "\\mathbb{I}", "JJ": "\\mathbb{J}", "KK": "\\mathbb{K}", "NN": "\\mathbb{N}", "Nat": "\\mathbb{N}", "PP": "\\mathbb{P}", "QQ": "\\mathbb{Q}", "RR": "\\mathbb{R}", "RRMN": "\\mathbb{R}^{M \\times N}", "SS": "\\mathbb{S}", "TT": "\\mathbb{T}", "UU": "\\mathbb{U}", "VV": "\\mathbb{V}", "WW": "\\mathbb{W}", "XX": "\\mathbb{X}", "YY": "\\mathbb{Y}", "ZZ": "\\mathbb{Z}", "ZERO": "\\mathbf{O}", "ERL": "\\overline{\\mathbb{R}}", "RERL": "(-\\infty, \\infty]", "LERL": "[-\\infty, \\infty)", "AAA": "\\mathcal{A}", "BBB": "\\mathcal{B}", "CCC": "\\mathcal{C}", "DDD": "\\mathcal{D}", "EEE": "\\mathcal{E}", "FFF": "\\mathcal{F}", "GGG": "\\mathcal{G}", "HHH": "\\mathcal{H}", "III": "\\mathcal{I}", "JJJ": "\\mathcal{J}", "KKK": "\\mathcal{K}", "LLL": "\\mathcal{L}", "MMM": "\\mathcal{M}", "NNN": "\\mathcal{N}", "OOO": "\\mathcal{O}", "PPP": "\\mathcal{P}", "QQQ": "\\mathcal{Q}", "RRR": "\\mathcal{R}", "SSS": "\\mathcal{S}", "TTT": "\\mathcal{T}", "UUU": "\\mathcal{U}", "VVV": "\\mathcal{V}", "WWW": "\\mathcal{W}", "XXX": "\\mathcal{X}", "YYY": "\\mathcal{Y}", "ZZZ": "\\mathcal{Z}", "Tau": "\\mathbf{\\mathcal{T}}", "Chi": "\\mathbf{\\mathcal{X}}", "Eta": "\\mathbf{\\mathcal{H}}", "Re": "\\operatorname{Re}", "Im": "\\operatorname{Im}", "bigO": "\\mathcal{O}", "smallO": "\\mathcal{o}", "NullSpace": "\\mathcal{N}", "ColSpace": "\\mathcal{C}", "RowSpace": "\\mathcal{R}", "Power": "\\mathop{\\mathcal{P}}", "LinTSpace": "\\mathcal{L}", "Range": "\\mathrm{R}", "Image": "\\mathrm{im}", "Kernel": "\\mathrm{ker}", "Span": "\\mathrm{span}", "Nullity": "\\mathrm{nullity}", "Dim": "\\mathrm{dim}", "Rank": "\\mathrm{rank}", "Trace": "\\mathrm{tr}", "Diag": "\\mathrm{diag}", "diag": "\\mathrm{diag}", "sgn": "\\mathrm{sgn}", "dom": "\\mathrm{dom}\\,", "range": "\\mathrm{range}\\,", "image": "\\mathrm{im}\\,", "nullspace": "\\mathrm{null}\\,", "epi": "\\mathrm{epi}\\,", "hypo": "\\mathrm{hypo}\\,", "sublevel": "\\mathrm{sublevel}", "superlevel": "\\mathrm{superlevel}", "contour": "\\mathrm{contour}", "supp": "\\mathrm{supp}", "dist": "\\mathrm{dist}", "opt": "\\mathrm{opt}", "succ": "\\mathrm{succ}", "SNR": "\\mathrm{SNR}", "RSNR": "\\mbox{R-SNR}", "rowsupp": "\\mathop{\\mathrm{rowsupp}}", "abs": "\\mathop{\\mathrm{abs}}", "erf": "\\mathop{\\mathrm{erf}}", "erfc": "\\mathop{\\mathrm{erfc}}", "Sub": "\\mathop{\\mathrm{Sub}}", "SSub": "\\mathop{\\mathrm{SSub}}", "Var": "\\mathop{\\mathrm{Var}}", "Cov": "\\mathop{\\mathrm{Cov}}", "AffineHull": "\\mathop{\\mathrm{aff}}", "ConvexHull": "\\mathop{\\mathrm{conv}}", "ConicHull": "\\mathop{\\mathrm{cone}}", "argmin": "\\mathrm{arg}\\,\\mathrm{min}", "argmax": "\\mathrm{arg}\\,\\mathrm{max}", "EmptySet": "\\varnothing", "card": "\\mathrm{card}\\,", "Forall": "\\; \\forall \\;", "ST": "\\: | \\:", "Gaussian": "\\mathcal{N}", "spark": "\\mathop{\\mathrm{spark}}", "ERC": "\\mathop{\\mathrm{ERC}}", "Maxcor": "\\mathop{\\mathrm{maxcor}}", "dag": "\\dagger", "Bracket": "\\left [ \\; \\right ]", "infimal": "\\;\\square\\;", "OneVec": "\\mathbf{1}", "ZeroVec": "\\mathbf{0}", "OneMat": "\\mathbb{1}", "Interior": ["\\mathring{#1}", 1], "Closure": ["\\overline{#1}", 1], "interior": "\\mathrm{int}\\,", "closure": "\\mathrm{cl}\\,", "boundary": "\\mathrm{bd}\\,", "frontier": "\\mathrm{fr}\\,", "diam": "\\mathrm{diam}\\,", "relint": "\\mathrm{ri}\\,", "relbd": "\\mathrm{relbd}\\,", "extreme": "\\mathrm{ext}\\,", "span": "\\mathrm{span}\\,", "affine": "\\mathrm{aff}\\,", "cone": "\\mathrm{cone}\\,", "convex": "\\mathrm{conv}\\,", "graph": "\\mathrm{gra}\\,", "kernel": "\\mathrm{ker}\\,", "dim": "\\mathrm{dim}\\,", "codim": "\\mathrm{codim}\\,", "nullity": "\\mathrm{nullity}\\,", "rank": "\\mathrm{rank}\\,", "prox": "\\mathrm{prox}", "best": "\\mathrm{best}", "ainterior": "\\mathrm{int}", "aclosure": "\\mathrm{cl}", "aboundary": "\\mathrm{bd}", "afrontier": "\\mathrm{fr}", "aextreme": "\\mathrm{ext}", "st": "\\mathrm{ST}", "ht": "\\mathrm{HT}", "bzero": "\\mathbf{0}", "bone": "\\mathbf{1}", "ba": "\\mathbf{a}", "bb": "\\mathbf{b}", "bc": "\\mathbf{c}", "bd": "\\mathbf{d}", "be": "\\mathbf{e}", "bf": "\\mathbf{f}", "bg": "\\mathbf{g}", "bh": "\\mathbf{h}", "bi": "\\mathbf{i}", "bj": "\\mathbf{j}", "bk": "\\mathbf{k}", "bl": "\\mathbf{l}", "bm": "\\mathbf{m}", "bn": "\\mathbf{n}", "bo": "\\mathbf{o}", "bp": "\\mathbf{p}", "bq": "\\mathbf{q}", "br": "\\mathbf{r}", "bs": "\\mathbf{s}", "bt": "\\mathbf{t}", "bu": "\\mathbf{u}", "bv": "\\mathbf{v}", "bw": "\\mathbf{w}", "bx": "\\mathbf{x}", "by": "\\mathbf{y}", "bz": "\\mathbf{z}", "bA": "\\mathbf{A}", "bB": "\\mathbf{B}", "bC": "\\mathbf{C}", "bD": "\\mathbf{D}", "bE": "\\mathbf{E}", "bF": "\\mathbf{F}", "bG": "\\mathbf{G}", "bH": "\\mathbf{H}", "bI": "\\mathbf{I}", "bJ": "\\mathbf{J}", "bK": "\\mathbf{K}", "bL": "\\mathbf{L}", "bM": "\\mathbf{M}", "bN": "\\mathbf{N}", "bO": "\\mathbf{O}", "bP": "\\mathbf{P}", "bQ": "\\mathbf{Q}", "bR": "\\mathbf{R}", "bS": "\\mathbf{S}", "bT": "\\mathbf{T}", "bU": "\\mathbf{U}", "bV": "\\mathbf{V}", "bW": "\\mathbf{W}", "bX": "\\mathbf{X}", "bY": "\\mathbf{Y}", "bZ": "\\mathbf{Z}", "bAAA": "\\mathbf{\\mathcal{A}}", "bBBB": "\\mathbf{\\mathcal{B}}", "bCCC": "\\mathbf{\\mathcal{C}}", "bDDD": "\\mathbf{\\mathcal{D}}", "bEEE": "\\mathbf{\\mathcal{E}}", "bFFF": "\\mathbf{\\mathcal{F}}", "bGGG": "\\mathbf{\\mathcal{G}}", "bHHH": "\\mathbf{\\mathcal{H}}", "bIII": "\\mathbf{\\mathcal{I}}", "bJJJ": "\\mathbf{\\mathcal{J}}", "bKKK": "\\mathbf{\\mathcal{K}}", "bLLL": "\\mathbf{\\mathcal{L}}", "bMMM": "\\mathbf{\\mathcal{M}}", "bNNN": "\\mathbf{\\mathcal{N}}", "bOOO": "\\mathbf{\\mathcal{O}}", "bPPP": "\\mathbf{\\mathcal{P}}", "bQQQ": "\\mathbf{\\mathcal{Q}}", "bRRR": "\\mathbf{\\mathcal{R}}", "bSSS": "\\mathbf{\\mathcal{S}}", "bTTT": "\\mathbf{\\mathcal{T}}", "bUUU": "\\mathbf{\\mathcal{U}}", "bVVV": "\\mathbf{\\mathcal{V}}", "bWWW": "\\mathbf{\\mathcal{W}}", "bXXX": "\\mathbf{\\mathcal{X}}", "bYYY": "\\mathbf{\\mathcal{Y}}", "bZZZ": "\\mathbf{\\mathcal{Z}}", "blambda": "\\pmb{\\lambda}"}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="9.6. Linear Constraints" href="linear_constraints.html" />
    <link rel="prev" title="9.4. Basic Duality" href="duality.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.google-analytics.com/analytics.js"></script>
<script>
                        window.ga = window.ga || function () {
                            (ga.q = ga.q || []).push(arguments) };
                        ga.l = +new Date;
                        ga('create', 'UA-214289683-2', 'auto');
                        ga('set', 'anonymizeIp', true);
                        ga('send', 'pageview');
                    </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Topics in Signal Processing</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   Overview
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Foundation
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../set_theory/intro.html">
   1. Set Theory
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../set_theory/sets.html">
     1.1. Sets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../set_theory/relations.html">
     1.2. Relations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../set_theory/functions.html">
     1.3. Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../set_theory/cardinality.html">
     1.4. Cardinality
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../set_theory/sequences.html">
     1.5. Sequences
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../set_theory/cartesian.html">
     1.6. General Cartesian Product
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../basic_real_analysis/chapter.html">
   2. Elementary Real Analysis
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../basic_real_analysis/real_line.html">
     2.1. Real Line
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../basic_real_analysis/topology.html">
     2.2. Topology of Real Line
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../basic_real_analysis/sequences.html">
     2.3. Sequences and Series
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../basic_real_analysis/erl.html">
     2.4. The Extended Real Line
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../basic_real_analysis/real_valued_functions.html">
     2.5. Real Valued Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../basic_real_analysis/real_functions.html">
     2.6. Real Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../basic_real_analysis/differentiability.html">
     2.7. Differentiable Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../basic_real_analysis/inequalities.html">
     2.8. Some Important Inequalities
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../metric_spaces/chapter.html">
   3. Metric Spaces
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../metric_spaces/intro.html">
     3.1. Introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../metric_spaces/topology.html">
     3.2. Metric Topology
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../metric_spaces/boundedness.html">
     3.3. Boundedness
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../metric_spaces/sequences.html">
     3.4. Sequences
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../metric_spaces/subspaces.html">
     3.5. Subspace Topology
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../metric_spaces/continuity.html">
     3.6. Functions and Continuity
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../metric_spaces/complete.html">
     3.7. Completeness
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../metric_spaces/compact.html">
     3.8. Compactness
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../metric_spaces/real_valued_functions.html">
     3.9. Real Valued Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../metric_spaces/discrete_space.html">
     3.10. Discrete Metric Space
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../metric_spaces/topics.html">
     3.11. Special Topics
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../la/chapter.html">
   4. Linear Algebra
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../la/matrices.html">
     4.1. Matrices I
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../la/vector_spaces.html">
     4.2. Vector Spaces
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../la/matrices_2.html">
     4.3. Matrices II
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../la/transformations.html">
     4.4. Linear Transformations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../la/normed_spaces.html">
     4.5. Normed Linear Spaces
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../la/inner_product_spaces.html">
     4.6. Inner Product Spaces
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../la/dual_spaces.html">
     4.7. Dual Spaces
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../la/euclidean.html">
     4.8. The Euclidean Space
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../la/matrices_3.html">
     4.9. Matrices III
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../la/evd.html">
     4.10. Eigen Values
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../la/svd.html">
     4.11. Singular Values
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../la/important_spaces.html">
     4.12. Important Vector Spaces
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../la/matrix_norms.html">
     4.13. Matrix Norms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../la/sequence_spaces.html">
     4.14. Sequence Spaces
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../la/affine.html">
     4.15. Affine Sets and Transformations
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../mv_calculus/chapter.html">
   5. Multivariate Calculus
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../mv_calculus/differentiation.html">
     5.1. Differentiation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../mv_calculus/frechet.html">
     5.2. Differentiation in Banach Spaces
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../randomness/chapter_prob.html">
   6. Probability
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../randomness/random_variables.html">
     6.1. Random Variables
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../randomness/univariate_distributions.html">
     6.2. Univariate Distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../randomness/inequalities.html">
     6.3. Basic Inequalities
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../randomness/two_vars.html">
     6.4. Two Variables
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../randomness/expectation.html">
     6.5. Expectation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../randomness/random_vectors.html">
     6.6. Random Vectors
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../randomness/gaussian_vec.html">
     6.7. Multivariate Gaussian Distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../randomness/subgaussian.html">
     6.8. Subgaussian Distributions
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../num_opt/chapter.html">
   7. Numerical Optimization
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../num_opt/opt_intro.html">
     7.1. Mathematical Optimization
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Convexity
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../convex_sets/intro.html">
   8. Convex Sets and Functions
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../convex_sets/real_spaces.html">
     8.1. Real Vector Spaces
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../convex_sets/convex.html">
     8.2. Convex Sets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../convex_sets/rn_subsets.html">
     8.3. Convex Subsets of
     <span class="math notranslate nohighlight">
      \(\RR^n\)
     </span>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../convex_sets/cone.html">
     8.4. Cones
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../convex_sets/cone_2.html">
     8.5. Cones II
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../convex_sets/cone_3.html">
     8.6. Cones III
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../convex_sets/generalized_inequality.html">
     8.7. Generalized Inequalities
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../convex_sets/convex_functions.html">
     8.8. Convex Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../convex_sets/differentiable.html">
     8.9. Differentiability and Convex Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../convex_sets/function_ops.html">
     8.10. Function Operations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../convex_sets/relint.html">
     8.11. Topology of Convex Sets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../convex_sets/separation.html">
     8.12. Separation Theorems
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../convex_sets/continuity.html">
     8.13. Continuity
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../convex_sets/recession_cones.html">
     8.14. Recession Cones
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../convex_sets/directional_derivatives.html">
     8.15. Directional Derivatives
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../convex_sets/subgradients.html">
     8.16. Subgradients
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../convex_sets/conjugate_functions.html">
     8.17. Conjugate Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../convex_sets/smoothness.html">
     8.18. Smoothness
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../convex_sets/infimal.html">
     8.19. Infimal Convolution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="chapter.html">
   9. Convex Optimization
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="cvxopt.html">
     9.1. Convex Optimization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="projection.html">
     9.2. Projection on Convex Sets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="recession_opt.html">
     9.3. Directions of Recession
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="duality.html">
     9.4. Basic Duality
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     9.5. Constrained Optimization I
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="linear_constraints.html">
     9.6. Linear Constraints
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="constrained_opt.html">
     9.7. Constrained Optimization II
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="lagrange_multipliers.html">
     9.8. Lagrange Multipliers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="lagrangian_duality.html">
     9.9. Lagrangian Duality
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="conjugate_duality.html">
     9.10. Conjugate Duality
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="linear_programming.html">
     9.11. Linear Programming
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="quadratic_programming.html">
     9.12. Quadratic Programming
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../subgradient_methods/chapter.html">
   10. Subgradient Methods
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../subgradient_methods/basic_subgradient.html">
     10.1. Basic Subgradient Method
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../proximal_operator/chapter.html">
   11. Proximal Algorithms
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../proximal_operator/prox_op.html">
     11.3. Proximal Mappings and Operators
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Sparsity
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ssm/chapter_ssm.html">
   12. Sparse Signal Models
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ssm/underdetermined.html">
     12.3. Underdetermined Linear Systems
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ssm/onb_sparsity.html">
     12.4. Sparsity in Orthonormal Bases
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ssm/srr.html">
     12.5. Sparse and Redundant Representations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ssm/dictionaries.html">
     12.6. Dictionaries
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ssm/compressive_sensing.html">
     12.7. Compressive Sensing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ssm/rip.html">
     12.8. Restricted Isometry Property
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ssm/dictionaries_2.html">
     12.9. Dictionaries II
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../compressive_sensing/chapter_compressive_sensing.html">
   13. Compressive Sensing
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../compressive_sensing/sensing_matrices.html">
     13.1. Sensing Matrices
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../sparse_approx/ch_sparse_approx.html">
   14. Sparse Approximation with Dictionaries
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
  <label for="toctree-checkbox-14">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../sparse_approx/stability.html">
     14.1. Stability of the Sparsest Solution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../sparse_approx/basis_pursuit_sa.html">
     14.2. Basis Pursuit
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../sparse_approx/omp_sa.html">
     14.3. Orthogonal Matching Pursuit
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../sparse_recovery/ch_sparse_recovery.html">
   15. Sparse Recovery from Compressive Measurements
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
  <label for="toctree-checkbox-15">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../sparse_recovery/stability_sr.html">
     15.1. Stability of the Sparsest Solution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../sparse_recovery/basis_pursuit_sr.html">
     15.2. Basis Pursuit
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../sparse_recovery/omp_cs.html">
     15.3. Orthogonal Matching Pursuit
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../sparse_recovery/cosamp_cs.html">
     15.4. Compressive Sampling Matching Pursuit
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../diclearn/ch_diclearn.html">
   16. Dictionary Learning
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
  <label for="toctree-checkbox-16">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../diclearn/intro_diclearn.html">
     16.1. Introduction
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Epilogue
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../notation.html">
   Notation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../bib.html">
   Bibliographic Notes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../genindex.html">
   Index
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/cvxopt/differentiable_objectives.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/shailesh1729/tisp"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/shailesh1729/tisp/issues/new?title=Issue%20on%20page%20%2Fcvxopt/differentiable_objectives.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#stationary-points">
   9.5.1. Stationary Points
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#first-order-optimality-criteria">
   9.5.2. First Order Optimality Criteria
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#nondifferentiability-at-the-boundary">
     9.5.2.1. Nondifferentiability at the Boundary
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#equality-constraints">
     9.5.2.2. Equality Constraints
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#nonnegative-orthant-constraints">
     9.5.2.3. Nonnegative Orthant Constraints
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#unit-sum-set-constraint">
     9.5.2.4. Unit Sum Set Constraint
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#unit-ball-constraint">
     9.5.2.5. Unit Ball Constraint
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#descent-directions-method">
   9.5.3. Descent Directions Method
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     9.5.3.1. Descent Directions Method
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#step-size-selection">
     9.5.3.2. Step Size Selection
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gradient-method">
   9.5.4. Gradient Method
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gradient-projection-method">
   9.5.5. Gradient Projection Method
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#convergence">
     9.5.5.1. Convergence
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Constrained Optimization I</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#stationary-points">
   9.5.1. Stationary Points
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#first-order-optimality-criteria">
   9.5.2. First Order Optimality Criteria
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#nondifferentiability-at-the-boundary">
     9.5.2.1. Nondifferentiability at the Boundary
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#equality-constraints">
     9.5.2.2. Equality Constraints
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#nonnegative-orthant-constraints">
     9.5.2.3. Nonnegative Orthant Constraints
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#unit-sum-set-constraint">
     9.5.2.4. Unit Sum Set Constraint
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#unit-ball-constraint">
     9.5.2.5. Unit Ball Constraint
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#descent-directions-method">
   9.5.3. Descent Directions Method
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     9.5.3.1. Descent Directions Method
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#step-size-selection">
     9.5.3.2. Step Size Selection
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gradient-method">
   9.5.4. Gradient Method
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gradient-projection-method">
   9.5.5. Gradient Projection Method
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#convergence">
     9.5.5.1. Convergence
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="constrained-optimization-i">
<span id="sec-opt-convex-differentiable-objective"></span><h1><span class="section-number">9.5. </span>Constrained Optimization I<a class="headerlink" href="#constrained-optimization-i" title="Permalink to this headline">Â¶</a></h1>
<p>In this section, we focus on objective functions of type
<span class="math notranslate nohighlight">\(f : \RR^n \to \RR\)</span> which are convex and differentiable.
Our goal is to minimize <span class="math notranslate nohighlight">\(f\)</span> over a convex set <span class="math notranslate nohighlight">\(C \subseteq \dom f\)</span>.</p>
<p>We also concern ourselves with functions of type <span class="math notranslate nohighlight">\(f : \VV \to \RERL\)</span>
where <span class="math notranslate nohighlight">\(\VV\)</span> is a Euclidean space and <span class="math notranslate nohighlight">\(f\)</span>
is FrÃ©chet differentiable over some open set <span class="math notranslate nohighlight">\(C \subseteq \dom f\)</span>.
For those results which concern with FrÃ©chet differentiable
functions, we request the readers to revise the concepts
of Gateaux and FrÃ©chet differentiability in
<a class="reference internal" href="../mv_calculus/frechet.html#sec-mvc-banach-diff"><span class="std std-ref">Differentiation in Banach Spaces</span></a>.</p>
<p>Main references for this section are
<span id="id1">[<a class="reference internal" href="../bib.html#id23" title="Amir Beck. Introduction to nonlinear optimization: Theory, algorithms, and applications with MATLAB. SIAM, 2014.">5</a>, <a class="reference internal" href="../bib.html#id27" title="Stephen P Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.">10</a>]</span>.</p>
<div class="section" id="stationary-points">
<h2><span class="section-number">9.5.1. </span>Stationary Points<a class="headerlink" href="#stationary-points" title="Permalink to this headline">Â¶</a></h2>
<p>We first look at functions which are differentiable over a
convex set. Here, we donâ€™t require that the function itself
be convex. Thus, we may not characterize the global
optimality of a point. However, we can still
characterize the local optimality of a point.</p>
<p>In <a class="reference internal" href="../num_opt/opt_intro.html#def-opt-stationary-point">Definition 7.6</a>, we defined
stationary points for a real valued function as
points where the gradient vanishes; i.e. <span class="math notranslate nohighlight">\(\nabla f(\bx) = \bzero\)</span>.</p>
<p>In this section, we wish to restrict the domain of feasible
points to a convex set <span class="math notranslate nohighlight">\(C\)</span> and consider the problem</p>
<div class="math notranslate nohighlight">
\[\begin{split}
&amp; \text{minimize }  &amp;  &amp; f(\bx) \\
&amp; \text{subject to } &amp; &amp; \bx \in C
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(f\)</span> is differentiable over the convex set <span class="math notranslate nohighlight">\(C\)</span>.</p>
<p>We now introduce the notion of stationary points for
the given optimization problem of minimizing <span class="math notranslate nohighlight">\(f\)</span> over <span class="math notranslate nohighlight">\(C\)</span>.</p>
<div class="proof definition admonition" id="def-opt-over-c-stationary-point">
<span id="index-0"></span><p class="admonition-title"><span class="caption-number">Definition 9.21 </span> (Stationary point for an optimization problem)</p>
<div class="definition-content section" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(f : \RR^n \to \RR\)</span> be a real valued function which
is differentiable over a convex set <span class="math notranslate nohighlight">\(C\)</span>.</p>
<p>Then, <span class="math notranslate nohighlight">\(\ba \in C\)</span> is called a <em>stationary point</em> of the
problem of minimizing <span class="math notranslate nohighlight">\(f\)</span> over <span class="math notranslate nohighlight">\(C\)</span> if</p>
<div class="math notranslate nohighlight">
\[
\nabla f(\ba)^T (\bx - \ba) \geq 0 \Forall \bx \in C.
\]</div>
</div>
</div><p>If <span class="math notranslate nohighlight">\(C = \RR^n\)</span>, then the condition will reduce to
<span class="math notranslate nohighlight">\(\nabla f(\ba) = \bzero\)</span>.</p>
<div class="proof theorem admonition" id="res-opt-over-c-local-min-stationary">
<p class="admonition-title"><span class="caption-number">Theorem 9.45 </span> (Local minimum points are stationary points)</p>
<div class="theorem-content section" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(f : \RR^n \to \RR\)</span> be a real valued function which
is differentiable over a convex set <span class="math notranslate nohighlight">\(C\)</span>.
Consider the optimization problem</p>
<div class="math notranslate nohighlight">
\[\begin{split}
&amp; \text{minimize }  &amp;  &amp; f(\bx) \\
&amp; \text{subject to } &amp; &amp; \bx \in C.
\end{split}\]</div>
<p>If <span class="math notranslate nohighlight">\(\ba \in C\)</span> is a local minimum, then <span class="math notranslate nohighlight">\(\ba\)</span> must be
a stationary point.</p>
</div>
</div><div class="proof admonition" id="proof">
<p>Proof. Let <span class="math notranslate nohighlight">\(\ba\)</span> be a local minimum and for contradiction assume
that it is not a stationary point.</p>
<ol>
<li><p>Then, there exists <span class="math notranslate nohighlight">\(\bx \in C\)</span> such that
<span class="math notranslate nohighlight">\(\nabla f(\ba)^T (\bx - \ba) &lt; 0\)</span>.</p></li>
<li><p>Let <span class="math notranslate nohighlight">\(t \in [0,1]\)</span> be a parameter.</p></li>
<li><p>Let <span class="math notranslate nohighlight">\(\bz_t = t \bx + (1-t) \ba\)</span>.</p></li>
<li><p>Since <span class="math notranslate nohighlight">\(C\)</span> is convex, hence <span class="math notranslate nohighlight">\(\bz_t \in C\)</span>.</p></li>
<li><p>Differentiating <span class="math notranslate nohighlight">\(f(\bz_t)\)</span> w.r.t. <span class="math notranslate nohighlight">\(t\)</span> at <span class="math notranslate nohighlight">\(t=0\)</span>, we obtain</p>
<div class="math notranslate nohighlight">
\[
   \left . \frac{d}{d t} f(\bz_t) \right |_{t=0} = \nabla f(\ba)^T (\bx - \ba) &lt; 0.
   \]</div>
</li>
<li><p>Thus, for small enough <span class="math notranslate nohighlight">\(t\)</span>, <span class="math notranslate nohighlight">\(f(\bz_t) &lt; f(\ba)\)</span>.</p></li>
<li><p>This contradicts the hypothesis that <span class="math notranslate nohighlight">\(\ba\)</span> is a local minimum.</p></li>
<li><p>Thus, all local minimum points must be stationary points.</p></li>
</ol>
</div>
<div class="proof example admonition" id="ex-opt-diff-obj-unconstrained-minimization">
<p class="admonition-title"><span class="caption-number">Example 9.6 </span> (Unconstrained minimization)</p>
<div class="example-content section" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(f : \RR^n \to \RR\)</span> be a real valued function which
is differentiable.
Consider the unconstrained optimization problem</p>
<div class="math notranslate nohighlight">
\[
\text{minimize }  f(\bx).
\]</div>
<p>In this case, the feasible set is <span class="math notranslate nohighlight">\(C = \RR^n\)</span>.</p>
<ol>
<li><p>If <span class="math notranslate nohighlight">\(\ba\)</span> is a stationary point for this problem, then</p>
<div class="math notranslate nohighlight">
\[
   \nabla f(\ba)^T (\bx - \ba) \geq 0 \Forall \bx \in \RR^n.
   \]</div>
</li>
<li><p>In particular, if we choose <span class="math notranslate nohighlight">\(\bx = \ba - \nabla f(\ba)\)</span>, we get</p>
<div class="math notranslate nohighlight">
\[
   \nabla f(\ba)^T (\bx - \ba) = \nabla f(\ba)^T (- \nabla f(\ba))
   = - \|  \nabla f(\ba) \|^2 \geq 0.
   \]</div>
</li>
<li><p>This is true only if <span class="math notranslate nohighlight">\(\nabla f(\ba) = \bzero\)</span>.</p></li>
<li><p>Thus, for unconstrained minimization, the gradient vanishes at stationary points.</p></li>
</ol>
</div>
</div><div class="proof theorem admonition" id="res-opt-over-c-stationary-orth-proj">
<p class="admonition-title"><span class="caption-number">Theorem 9.46 </span> (Stationary point as an orthogonal projection)</p>
<div class="theorem-content section" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(f : \RR^n \to \RR\)</span> be a real valued function which
is differentiable over a convex set <span class="math notranslate nohighlight">\(C\)</span>.
Consider the optimization problem</p>
<div class="math notranslate nohighlight">
\[\begin{split}
&amp; \text{minimize }  &amp;  &amp; f(\bx) \\
&amp; \text{subject to } &amp; &amp; \bx \in C.
\end{split}\]</div>
<p>Let <span class="math notranslate nohighlight">\(s &gt; 0\)</span>.
Then <span class="math notranslate nohighlight">\(\ba \in C\)</span> is a stationary point of the optimization
problem if and only if</p>
<div class="math notranslate nohighlight">
\[
\ba = P_C (\ba - s \nabla f(\ba)).
\]</div>
</div>
</div><div class="proof admonition" id="proof">
<p>Proof. Recall from <a class="reference internal" href="projection.html#res-cvx-projection-characterization">Theorem 9.7</a> that
<span class="math notranslate nohighlight">\(\bz \in C\)</span> is the projection of <span class="math notranslate nohighlight">\(\bx\)</span> if and only if</p>
<div class="math notranslate nohighlight">
\[
\langle \by - \bz, \bx - \bz \rangle \leq 0 \Forall \by \in C.
\]</div>
<ol>
<li><p>Replace <span class="math notranslate nohighlight">\(\bz = \ba\)</span> and <span class="math notranslate nohighlight">\(\bx  = \ba - s \nabla f(\ba)\)</span>. We get</p>
<div class="math notranslate nohighlight">
\[\begin{split}
   &amp; \langle \by - \ba, \ba - s \nabla f(\ba) - \ba \rangle \leq 0 \Forall \by \in C\\
   &amp; \iff  s\langle \by - \ba, \nabla f(\ba) \rangle \geq 0 \Forall \by \in C\\
   &amp; \iff \nabla f(\ba)^T (\by - \ba) \geq 0 \Forall \by \in C.
   \end{split}\]</div>
</li>
<li><p>But this is the same condition as the definition for a stationary point.</p></li>
</ol>
</div>
</div>
<div class="section" id="first-order-optimality-criteria">
<h2><span class="section-number">9.5.2. </span>First Order Optimality Criteria<a class="headerlink" href="#first-order-optimality-criteria" title="Permalink to this headline">Â¶</a></h2>
<p>We now pay our attention to the case where <span class="math notranslate nohighlight">\(f\)</span> is convex
as well as differentiable. In this case, a point is a global
optimal point if and only if it is a stationary point.</p>
<div class="proof theorem admonition" id="res-cvxopt-diff-convex-optimal-criterion">
<p class="admonition-title"><span class="caption-number">Theorem 9.47 </span> (Optimality criterion for differentiable objective function)</p>
<div class="theorem-content section" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(f : \RR^n \to \RR\)</span> be a differentiable convex function.
Let <span class="math notranslate nohighlight">\(C \subseteq \dom f\)</span> be a convex set.
Consider the minimization problem</p>
<div class="math notranslate nohighlight">
\[\begin{split}
&amp; \text{minimize }  &amp;  &amp; f(\bx) \\
&amp; \text{subject to } &amp; &amp; \bx \in C.
\end{split}\]</div>
<p>Then, <span class="math notranslate nohighlight">\(\bx \in C\)</span> is an optimal point if and only if</p>
<div class="math notranslate nohighlight" id="equation-eq-cvx-opt-diff-opt-criterion">
<span class="eqno">(9.8)<a class="headerlink" href="#equation-eq-cvx-opt-diff-opt-criterion" title="Permalink to this equation">Â¶</a></span>\[\nabla f(\bx)^T (\by - \bx) \geq 0 \Forall \by \in C.\]</div>
<p>In other words, <span class="math notranslate nohighlight">\(\bx\)</span> is optimal if and only if it is a stationary point.</p>
</div>
</div><div class="proof admonition" id="proof">
<p>Proof. By <a class="reference internal" href="../convex_sets/differentiable.html#res-cvxf-gradient-convexity-relation">Theorem 8.98</a></p>
<div class="math notranslate nohighlight">
\[
f(\by) \geq f(\bx) + \nabla f(\bx)^T (\by - \bx)
\]</div>
<p>for every <span class="math notranslate nohighlight">\(\bx, \by \in \dom f\)</span>.</p>
<p>Assume that some <span class="math notranslate nohighlight">\(\bx \in C\)</span> satisfies the optimality criterion
in <a class="reference internal" href="#equation-eq-cvx-opt-diff-opt-criterion">(9.8)</a>.</p>
<ol>
<li><p>Let <span class="math notranslate nohighlight">\(\by \in C\)</span>.</p></li>
<li><p>Then, by differentiability</p>
<div class="math notranslate nohighlight">
\[
   f(\by) \geq f(\bx) + \nabla f(\bx)^T (\by - \bx).
   \]</div>
</li>
<li><p>By hypothesis <span class="math notranslate nohighlight">\(\nabla f(\bx)^T (\by - \bx) \geq 0\)</span>.</p></li>
<li><p>Thus, <span class="math notranslate nohighlight">\(f(\by) \geq f(\bx)\)</span>.</p></li>
<li><p>Since this is true for every <span class="math notranslate nohighlight">\(\by \in C\)</span>, hence
<span class="math notranslate nohighlight">\(\bx\)</span> is an optimal point for the minimization problem.</p></li>
</ol>
<p>Now for the converse, assume that <span class="math notranslate nohighlight">\(\bx \in C\)</span> is an optimal point.</p>
<ol>
<li><p>For contradiction, assume that <a class="reference internal" href="#equation-eq-cvx-opt-diff-opt-criterion">(9.8)</a>
doesnâ€™t hold.</p></li>
<li><p>Then, there exists <span class="math notranslate nohighlight">\(\by \in C\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
   \nabla f(\bx)^T (\by - \bx) &lt; 0.
   \]</div>
</li>
<li><p>Let <span class="math notranslate nohighlight">\(t \in [0, 1]\)</span> be a parameter.</p></li>
<li><p>Let <span class="math notranslate nohighlight">\(\bz(t) = t \by + (1-t) \bx\)</span>.</p></li>
<li><p>Since <span class="math notranslate nohighlight">\(C\)</span> is convex, hence <span class="math notranslate nohighlight">\(\bz(t) \in C\)</span>.</p></li>
<li><p>Differentiating <span class="math notranslate nohighlight">\(f(\bz(t))\)</span> w.r.t. <span class="math notranslate nohighlight">\(t\)</span> at <span class="math notranslate nohighlight">\(t=0\)</span>, we obtain</p>
<div class="math notranslate nohighlight">
\[
   \left . \frac{d}{d t} f(\bz(t)) \right |_{t=0} = \nabla f(\bx)^T (\by - \bx) &lt; 0.
   \]</div>
</li>
<li><p>Thus, for small enough <span class="math notranslate nohighlight">\(t\)</span>, <span class="math notranslate nohighlight">\(f(\bz(t)) &lt; f(\bx)\)</span>.</p></li>
<li><p>Thus, <span class="math notranslate nohighlight">\(\bx\)</span> cannot be an optimal point for the minimization problem.</p></li>
<li><p>This contradicts our hypothesis that <span class="math notranslate nohighlight">\(\bx\)</span> is an optimal point.</p></li>
<li><p>Hence, <a class="reference internal" href="#equation-eq-cvx-opt-diff-opt-criterion">(9.8)</a> must hold for every <span class="math notranslate nohighlight">\(\by \in C\)</span>.</p></li>
</ol>
</div>
<div class="proof theorem admonition" id="res-cvxopt-diff-convex-optimal-unconstrained">
<p class="admonition-title"><span class="caption-number">Theorem 9.48 </span> (Optimality criterion for unconstrained problem with differentiable objective function)</p>
<div class="theorem-content section" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(f : \RR^n \to \RR\)</span> be a differentiable convex function.
Consider the unconstrained minimization problem</p>
<div class="math notranslate nohighlight">
\[
\text{minimize } f(\bx)
\]</div>
<p>Then, <span class="math notranslate nohighlight">\(\bx \in \RR^n\)</span> is an optimal point if and only if
<span class="math notranslate nohighlight">\(\nabla f(\bx) = \bzero\)</span>.</p>
</div>
</div><div class="proof admonition" id="proof">
<p>Proof. In this case, the set of feasible points is <span class="math notranslate nohighlight">\(C = \dom f\)</span>.</p>
<ol>
<li><p>Since <span class="math notranslate nohighlight">\(f\)</span> is differentiable, hence <span class="math notranslate nohighlight">\(C\)</span> is an open set.</p></li>
<li><p>By <a class="reference internal" href="#res-cvxopt-diff-convex-optimal-criterion">Theorem 9.47</a>,
<span class="math notranslate nohighlight">\(\bx\)</span> is an optimal point if and only if</p>
<div class="math notranslate nohighlight">
\[
   \nabla f(\bx)^T (\by - \bx) \geq 0 \Forall \by \in C.
   \]</div>
</li>
<li><p>If <span class="math notranslate nohighlight">\(\nabla f(\bx) = \bzero\)</span>, then this inequality is satisfied.
Hence, <span class="math notranslate nohighlight">\(\bx\)</span> must be an optimal point.</p></li>
<li><p>Now, assume that <span class="math notranslate nohighlight">\(\bx\)</span> is an optimal point.</p></li>
<li><p>Since <span class="math notranslate nohighlight">\(\bx \in C\)</span> and <span class="math notranslate nohighlight">\(C\)</span> is open, hence there exists
a closed ball <span class="math notranslate nohighlight">\(B[\bx, r] \subseteq C\)</span>.</p></li>
<li><p>Let <span class="math notranslate nohighlight">\(\by = \bx - t \nabla f(\bx)\)</span>.</p></li>
<li><p>For sufficiently small <span class="math notranslate nohighlight">\(t &gt; 0\)</span>, <span class="math notranslate nohighlight">\(\by \in B[\bx, r] \subseteq C\)</span>.</p></li>
<li><p>Then,</p>
<div class="math notranslate nohighlight">
\[
   \nabla f(\bx)^T (\by - \bx)  = \nabla f(\bx)^T (-t  \nabla f(\bx))
   = -t \|  \nabla f(\bx) \|_2^2 \geq 0
   \]</div>
<p>must hold true for <span class="math notranslate nohighlight">\(t &gt; 0\)</span>.</p>
</li>
<li><p>This means that <span class="math notranslate nohighlight">\(\|  \nabla f(\bx) \|_2 \leq 0\)</span> must be true.</p></li>
<li><p>Thus <span class="math notranslate nohighlight">\(\nabla f(\bx) = \bzero\)</span> must be true.</p></li>
</ol>
</div>
<div class="section" id="nondifferentiability-at-the-boundary">
<h3><span class="section-number">9.5.2.1. </span>Nondifferentiability at the Boundary<a class="headerlink" href="#nondifferentiability-at-the-boundary" title="Permalink to this headline">Â¶</a></h3>
<p>There are some specific results available for the unconstrained minimization
of a convex function <span class="math notranslate nohighlight">\(f\)</span> is not differentiable everywhere in its domain.
If <span class="math notranslate nohighlight">\(\dom f\)</span> is not open, then <span class="math notranslate nohighlight">\(f\)</span> may be differentiable
at <span class="math notranslate nohighlight">\(\interior \dom f\)</span> but is not differentiable
at the boundary points <span class="math notranslate nohighlight">\((\dom f) \setminus (\interior dom f)\)</span>.
The key questions are</p>
<ol class="simple">
<li><p>Under what conditions, the minimizer of <span class="math notranslate nohighlight">\(f\)</span> is
a point of differentiability.</p></li>
<li><p>Under what conditions, the minimizer of <span class="math notranslate nohighlight">\(f\)</span> may
be at a point of nondifferentiability.</p></li>
</ol>
<div class="proof theorem admonition" id="res-cvxopt-diff-zero-grad-minimizer">
<p class="admonition-title"><span class="caption-number">Theorem 9.49 </span> (Zero gradient implies minimizer)</p>
<div class="theorem-content section" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(f: \VV \to \RERL\)</span> with <span class="math notranslate nohighlight">\(S = \dom f\)</span> be a
proper convex function which is differentiable
over some open set <span class="math notranslate nohighlight">\(U \subseteq S\)</span>.
If <span class="math notranslate nohighlight">\(\nabla f(\bx) = \bzero\)</span> at some <span class="math notranslate nohighlight">\(\bx \in U\)</span>, then
<span class="math notranslate nohighlight">\(\bx\)</span> is one of the minimizers for <span class="math notranslate nohighlight">\(f\)</span>.</p>
</div>
</div><div class="proof admonition" id="proof">
<p>Proof. Recall from <a class="reference internal" href="../convex_sets/subgradients.html#res-cvxf-subdiff-grad">Theorem 8.220</a> that
<span class="math notranslate nohighlight">\(f\)</span> is differentiable at <span class="math notranslate nohighlight">\(\bx\)</span> if and only if</p>
<div class="math notranslate nohighlight">
\[
\partial f(\bx) = \{ \nabla f (\bx) \}.
\]</div>
<ol>
<li><p>Assume that <span class="math notranslate nohighlight">\(f\)</span> is differentiable at <span class="math notranslate nohighlight">\(\bx \in U\)</span>
and <span class="math notranslate nohighlight">\(\nabla f(\bx) = \bzero\)</span>.</p></li>
<li><p>Then <span class="math notranslate nohighlight">\(\nabla f (\bx)\)</span> is the one and only subgradient of
<span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(\bx\)</span>.</p></li>
<li><p>Due to subgradient inequality</p>
<div class="math notranslate nohighlight">
\[
   f(\by) \geq f(\bx) + \langle \by - \bx, \nabla f(\bx) \rangle \Forall \by \in \VV.
   \]</div>
</li>
<li><p>Since <span class="math notranslate nohighlight">\(\nabla f(\bx) = \bzero\)</span>, hence</p>
<div class="math notranslate nohighlight">
\[
   f(\by) \geq f(\bx) \Forall \by \in \VV.
   \]</div>
</li>
<li><p>Thus <span class="math notranslate nohighlight">\(f\)</span> attains a minimum at <span class="math notranslate nohighlight">\(\bx\)</span> and <span class="math notranslate nohighlight">\(\bx\)</span> is one of
its minimizers.</p></li>
</ol>
</div>
<p>Next we consider the special case of the
minimization of a convex function <span class="math notranslate nohighlight">\(f\)</span> which is
differentiable in the interior of its domain
but the gradient never vanishes. In this
case, if a minimizer for <span class="math notranslate nohighlight">\(f\)</span> exists, it must be
at the boundary of its domain.</p>
<div class="proof theorem admonition" id="res-cvxopt-minimizer-nondifferentiability">
<p class="admonition-title"><span class="caption-number">Theorem 9.50 </span> (Minimizers at points of nondifferentiability)</p>
<div class="theorem-content section" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(f: \VV \to \RERL\)</span> with <span class="math notranslate nohighlight">\(S = \dom f\)</span> be a
proper convex function which is differentiable
over some open set <span class="math notranslate nohighlight">\(U \subseteq S\)</span>
and not differentiable over <span class="math notranslate nohighlight">\(S \setminus U\)</span>.
Assume that <span class="math notranslate nohighlight">\(f\)</span> attains its minimum value at
some <span class="math notranslate nohighlight">\(\ba \in S\)</span>; i.e., a minimizer of <span class="math notranslate nohighlight">\(f\)</span>
exists. If <span class="math notranslate nohighlight">\(\nabla f(\bx) \neq \bzero\)</span>
at every <span class="math notranslate nohighlight">\(\bx \in U\)</span>,
then the minimizer must be at some point of nondifferentiability.
In other words, <span class="math notranslate nohighlight">\(\ba \in S \setminus U\)</span>.</p>
</div>
</div><div class="proof admonition" id="proof">
<p>Proof. We are given that there exists a is a minimizer of <span class="math notranslate nohighlight">\(f\)</span>
and for every <span class="math notranslate nohighlight">\(\bx \in U\)</span>, <span class="math notranslate nohighlight">\(\nabla f(\bx) \neq \bzero\)</span>.</p>
<ol>
<li><p>Pick any <span class="math notranslate nohighlight">\(\bx \in U\)</span>.</p></li>
<li><p>We need to show that <span class="math notranslate nohighlight">\(\bx\)</span> cannot be a minimizer.</p></li>
<li><p>For contradiction, assume that <span class="math notranslate nohighlight">\(\bx\)</span> is a minimizer.</p></li>
<li><p>By subgradient inequality</p>
<div class="math notranslate nohighlight">
\[
   f(\by) \geq f(\bx) + \langle \by - \bx, \nabla f(\bx) \rangle \Forall \by \in \VV.
   \]</div>
</li>
<li><p>Since <span class="math notranslate nohighlight">\(\bx\)</span> is a minimizer, hence we must have</p>
<div class="math notranslate nohighlight">
\[
   \langle \by - \bx, \nabla f(\bx) \rangle \geq 0 \Forall \by \in \VV.
   \]</div>
</li>
<li><p>Let <span class="math notranslate nohighlight">\(\by = \bx - \nabla f(\bx)\)</span>.</p></li>
<li><p>Then</p>
<div class="math notranslate nohighlight">
\[
   \langle \by - \bx, \nabla f(\bx) \rangle 
   = \langle - \nabla f(\bx), \nabla f(\bx) \rangle
   = - \| \nabla f(\bx) \|^2 &lt; 0.
   \]</div>
</li>
<li><p>This contradicts our assumption that <span class="math notranslate nohighlight">\(\bx\)</span> is a minimizer.</p></li>
<li><p>Hence if <span class="math notranslate nohighlight">\(\ba\)</span> is a minimizer of <span class="math notranslate nohighlight">\(f\)</span> then <span class="math notranslate nohighlight">\(\ba \in S \setminus U\)</span>.</p></li>
</ol>
</div>
<p>We next show how the condition in <a class="reference internal" href="#equation-eq-cvx-opt-diff-opt-criterion">(9.8)</a>
simplifies for specific optimization problem structures.</p>
</div>
<div class="section" id="equality-constraints">
<h3><span class="section-number">9.5.2.2. </span>Equality Constraints<a class="headerlink" href="#equality-constraints" title="Permalink to this headline">Â¶</a></h3>
<div class="proof theorem admonition" id="res-cvxopt-diff-obj-linear-constraints">
<p class="admonition-title"><span class="caption-number">Theorem 9.51 </span> (Differentiable objective minimization with equality constraints)</p>
<div class="theorem-content section" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(f : \RR^n \to \RR\)</span> be a differentiable convex function
with <span class="math notranslate nohighlight">\(\dom f = \RR^n\)</span>.
Consider the minimization problem:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
&amp; \text{minimize }  &amp;  &amp; f(\bx) \\
&amp; \text{subject to } &amp; &amp; \bA \bx = \bb
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\bA \in \RR^{p \times n}\)</span> and <span class="math notranslate nohighlight">\(\bb \in \RR^p\)</span> represent
<span class="math notranslate nohighlight">\(p\)</span> linear equality constraints.
Assume that the problem is feasible.</p>
<p>Then, <span class="math notranslate nohighlight">\(\bx\)</span> is an optimal point for the minimization problem
if and only if there exists a vector <span class="math notranslate nohighlight">\(\bv \in \RR^p\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
\nabla f (\bx) + \bA^T \bv = \bzero.
\]</div>
</div>
</div><div class="proof admonition" id="proof">
<p>Proof. The feasible set is given by <span class="math notranslate nohighlight">\(C = \{ \bx \ST \bA \bx = \bb \}\)</span>.
We recall from <a class="reference internal" href="#res-cvxopt-diff-convex-optimal-criterion">Theorem 9.47</a>, that
<span class="math notranslate nohighlight">\(\bx\)</span> is an optimal point if and only if
<span class="math notranslate nohighlight">\(\nabla f(\bx)^T (\by - \bx) \geq 0 \Forall \by \in C\)</span>.</p>
<ol>
<li><p>Thus, <span class="math notranslate nohighlight">\(\bx\)</span> is feasible if and only if
<span class="math notranslate nohighlight">\(\nabla f(\bx)^T (\by - \bx) \geq 0\)</span> for every <span class="math notranslate nohighlight">\(\by\)</span> satisfying
<span class="math notranslate nohighlight">\(\bA \by = \bb\)</span>.</p></li>
<li><p>Since both <span class="math notranslate nohighlight">\(\bx\)</span> and <span class="math notranslate nohighlight">\(\by\)</span> are feasible, hence <span class="math notranslate nohighlight">\(\bA (\by - \bx) = \bzero\)</span>.</p></li>
<li><p>Thus, <span class="math notranslate nohighlight">\(\bz = \by - \bx \in \NullSpace(\bA)\)</span>.</p></li>
<li><p>In fact, <span class="math notranslate nohighlight">\(\by \in C\)</span> if and only if <span class="math notranslate nohighlight">\(\by = \bx + \bz\)</span> for some
<span class="math notranslate nohighlight">\(\bz \in \NullSpace(\bA)\)</span>.</p></li>
<li><p>Thus, the optimality criteria reduces to
<span class="math notranslate nohighlight">\(\nabla f(\bx)^T \bz \geq 0\)</span> for every <span class="math notranslate nohighlight">\(\bz \in \NullSpace(\bA)\)</span>.</p></li>
<li><p>Note that <span class="math notranslate nohighlight">\(\nabla f(\bx)^T \bz\)</span> is a linear function of <span class="math notranslate nohighlight">\(\bz\)</span>
as <span class="math notranslate nohighlight">\(\nabla f(\bx)\)</span> is a fixed vector.</p></li>
<li><p>If a linear function is nonnegative on a subspace, then it must
be identically zero on the subspace.</p></li>
<li><p>Thus, <span class="math notranslate nohighlight">\(\nabla f(\bx)^T \bz = 0\)</span> for every <span class="math notranslate nohighlight">\(\bz \in \NullSpace(\bA)\)</span>.</p></li>
<li><p>In other words, <span class="math notranslate nohighlight">\(\bx\)</span> is optimal if and only if
<span class="math notranslate nohighlight">\(\nabla f(\bx) \perp \NullSpace(\bA)\)</span>.</p></li>
<li><p>Recall that <span class="math notranslate nohighlight">\(\NullSpace(\bA)^{\perp} = \ColSpace(\bA^T)\)</span>;
i.e., the null space of <span class="math notranslate nohighlight">\(\bA\)</span> is orthogonal complement of the
column space (range) of <span class="math notranslate nohighlight">\(\bA^T\)</span>.</p></li>
<li><p>Thus, <span class="math notranslate nohighlight">\(\bx\)</span> is optimal if and only if
<span class="math notranslate nohighlight">\(\nabla f(\bx) \in \ColSpace(\bA^T)\)</span>.</p></li>
<li><p>In other words, <span class="math notranslate nohighlight">\(\bx\)</span> is optimal if and only if there exists
a vector <span class="math notranslate nohighlight">\(\bv \in \RR^p\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
   \nabla f (\bx) + \bA^T \bv = \bzero.
   \]</div>
</li>
</ol>
</div>
<p>This result is a Lagrange multiplier optimality condition to be
discussed in more detail in later sections.</p>
</div>
<div class="section" id="nonnegative-orthant-constraints">
<h3><span class="section-number">9.5.2.3. </span>Nonnegative Orthant Constraints<a class="headerlink" href="#nonnegative-orthant-constraints" title="Permalink to this headline">Â¶</a></h3>
<div class="proof example admonition" id="ex-cvxopt-diff-obj-nng-orthant">
<p class="admonition-title"><span class="caption-number">Example 9.7 </span> (Differentiable objective minimization over nonnegative orthant)</p>
<div class="example-content section" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(f : \RR^n \to \RR\)</span> be a differentiable convex function
with <span class="math notranslate nohighlight">\(\dom f = \RR^n\)</span>.
Consider the minimization problem:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
&amp; \text{minimize }  &amp;  &amp; f(\bx) \\
&amp; \text{subject to } &amp; &amp; \bx \succeq \bzero.
\end{split}\]</div>
<ol>
<li><p>The feasible set is the nonnegative orthant <span class="math notranslate nohighlight">\(\RR^n_+\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\bx\)</span> is optimal if and only if
<span class="math notranslate nohighlight">\(\bx \succeq \bzero\)</span> and
<span class="math notranslate nohighlight">\(\nabla f (\bx)^T (\by - \bx) \geq 0\)</span> for every <span class="math notranslate nohighlight">\(\by \succeq \bzero\)</span>.</p></li>
<li><p>The term <span class="math notranslate nohighlight">\(\nabla f (\bx)^T \by\)</span> is unbounded below on <span class="math notranslate nohighlight">\(\by \in \RR^n_+\)</span>
unless <span class="math notranslate nohighlight">\(\nabla f (\bx) \in \RR^n_+\)</span>.</p></li>
<li><p>Thus, <span class="math notranslate nohighlight">\(\nabla f (\bx)\)</span> must be nonnegative.</p></li>
<li><p>Then, the minimum value for <span class="math notranslate nohighlight">\(\nabla f (\bx)^T \by\)</span> is 0.</p></li>
<li><p>Consequently, the optimality condition reduces to
<span class="math notranslate nohighlight">\(-\nabla f (\bx)^T \bx \geq 0\)</span>
or <span class="math notranslate nohighlight">\(\nabla f (\bx)^T \bx \leq 0\)</span>.</p></li>
<li><p>But <span class="math notranslate nohighlight">\(\bx \succeq \bzero\)</span> and <span class="math notranslate nohighlight">\(\nabla f (\bx) \succeq \bzero\)</span>.</p></li>
<li><p>Thus, we must have <span class="math notranslate nohighlight">\(\nabla f (\bx)^T \bx = 0\)</span>.`</p></li>
<li><p>We note that</p>
<div class="math notranslate nohighlight">
\[
   \nabla f (\bx)^T \bx = \sum_{i=1}^n (\nabla f (\bx))_i x_i.
   \]</div>
</li>
<li><p>Thus, it is a sum of products of nonnegative numbers.</p></li>
<li><p>So each term in the sum must be 0.</p></li>
<li><p>Thus, <span class="math notranslate nohighlight">\((\nabla f (\bx))_i x_i = 0\)</span> must hold true for every
<span class="math notranslate nohighlight">\(i=1,\dots,n\)</span>.</p></li>
<li><p>Thus, the optimality condition can be rephrased as</p>
<div class="math notranslate nohighlight">
\[
   \bx \succeq \bzero
   \text{ and }
   \nabla f (\bx) \succeq \bzero
   \text{ and }
   (\nabla f (\bx))_i x_i = 0 \Forall i=1,\dots,n.
   \]</div>
</li>
</ol>
<p>The condition <span class="math notranslate nohighlight">\((\nabla f (\bx))_i x_i = 0\)</span> for every <span class="math notranslate nohighlight">\(i\)</span> is known
as <em>complementarity</em>. It means that for every <span class="math notranslate nohighlight">\(i\)</span> either
<span class="math notranslate nohighlight">\(x_i\)</span> or <span class="math notranslate nohighlight">\((\nabla f (\bx))_i\)</span> or both must be 0.
In other words, both <span class="math notranslate nohighlight">\(x_i\)</span> and <span class="math notranslate nohighlight">\((\nabla f (\bx))_i\)</span> cannot be
nonzero at the same time.</p>
<p>Thus, the sparsity patterns of <span class="math notranslate nohighlight">\(\bx\)</span> and <span class="math notranslate nohighlight">\(\nabla f (\bx)\)</span> are
<em>complementary</em>. In other words,</p>
<div class="math notranslate nohighlight">
\[
\supp (\bx) \cap \supp (\nabla f (\bx)) = \EmptySet
\]</div>
<p>where <span class="math notranslate nohighlight">\(\supp (\bx)\)</span> denotes the index set of nonzero entries of <span class="math notranslate nohighlight">\(\bx\)</span>.</p>
</div>
</div></div>
<div class="section" id="unit-sum-set-constraint">
<h3><span class="section-number">9.5.2.4. </span>Unit Sum Set Constraint<a class="headerlink" href="#unit-sum-set-constraint" title="Permalink to this headline">Â¶</a></h3>
<div class="proof example admonition" id="ex-cvxopt-diff-obj-unit-sum-set">
<p class="admonition-title"><span class="caption-number">Example 9.8 </span> (Minimization over unit sum set)</p>
<div class="example-content section" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(f : \RR^n \to \RR\)</span> be a differentiable convex function
with <span class="math notranslate nohighlight">\(\dom f = \RR^n\)</span>.
Consider the minimization problem:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
&amp; \text{minimize }  &amp;  &amp; f(\bx) \\
&amp; \text{subject to } &amp; &amp; \bone^T \bx  = 1.
\end{split}\]</div>
<ol>
<li><p>The feasible set is given by</p>
<div class="math notranslate nohighlight">
\[
   C = \{ \bx \in \RR^n \ST \bone^T \bx = 1 \} 
    = \{ \bx \in \RR^n \ST \sum_{i=1}^n x_i = 1 \}.
   \]</div>
</li>
<li><p>This set is also known as the <em>unit sum set</em>.</p></li>
<li><p>A point <span class="math notranslate nohighlight">\(\ba \in C\)</span> is optimal if and only if</p>
<div class="math notranslate nohighlight">
\[
   \nabla f(\ba)^T (\bx - \ba) \geq 0 \Forall \bx \in C.
   \]</div>
<p>Let us call this condition (I).</p>
</li>
<li><p>This is equivalent to the condition:</p>
<div class="math notranslate nohighlight">
\[
   \frac{\partial f}{\partial x_1} (\ba) 
   =  \frac{\partial f}{\partial x_2} (\ba)
   = \dots 
   = \frac{\partial f}{\partial x_n} (\ba).
   \]</div>
<p>Let us call this condition (II).</p>
</li>
<li><p>We shall show that (I) and (II) are equivalent.</p></li>
</ol>
<p>We first show that (II) implies (I).</p>
<ol>
<li><p>Assume that some <span class="math notranslate nohighlight">\(\ba \in C\)</span> satisfies (II)  with</p>
<div class="math notranslate nohighlight">
\[
   \alpha = \frac{\partial f}{\partial x_1} (\ba) 
   =  \frac{\partial f}{\partial x_2} (\ba)
   = \dots 
   = \frac{\partial f}{\partial x_n} (\ba).
   \]</div>
</li>
<li><p>Then, for any <span class="math notranslate nohighlight">\(\bx \in C\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
   \nabla f(\ba)^T (\bx - \ba)
   &amp;= \sum_{i=1}^n \frac{\partial f}{\partial x_i} (\ba) (x_i - a_i) \\
   &amp;= \alpha  \sum_{i=1}^n (x_i - a_i) \\
   &amp;= \alpha (\sum_{i=1}^n x_i - \sum_{i=1}^n a_i) \\
   &amp;= \alpha (1 - 1) = 0. 
   \end{split}\]</div>
</li>
<li><p>Thus, we see that <span class="math notranslate nohighlight">\(\nabla f(\ba)^T (\bx - \ba) \geq 0\)</span> indeed
and (I) is satisfied.</p></li>
</ol>
<p>Now, assume that (I) is satisfied for some <span class="math notranslate nohighlight">\(\ba \in C\)</span>.</p>
<ol>
<li><p>For contradiction, assume that <span class="math notranslate nohighlight">\(\ba\)</span> doesnâ€™t satisfy (II).</p></li>
<li><p>Then, their exist <span class="math notranslate nohighlight">\(i,j \in [1,\dots,n]\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
   \frac{\partial f}{\partial x_i} (\ba) 
   &gt; \frac{\partial f}{\partial x_j} (\ba).
   \]</div>
</li>
<li><p>Pick a vector <span class="math notranslate nohighlight">\(\bx \in C\)</span> with following definition</p>
<div class="math notranslate nohighlight">
\[\begin{split}
   x_k = \begin{cases}
   a_k &amp; k \notin \{i, j \} \\
   a_k - 1 &amp; k = i \\
   a_k + 1 &amp; k = j.
   \end{cases}
   \end{split}\]</div>
</li>
<li><p>Note that</p>
<div class="math notranslate nohighlight">
\[
   \sum_{i=1}^n x_k = \sum_{i=1}^n a_k = 1.
   \]</div>
<p>Thus, <span class="math notranslate nohighlight">\(\bx \in C\)</span> holds.</p>
</li>
<li><p>Now,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
   \nabla f(\ba)^T (\bx - \ba)
   &amp;= \sum_{k=1}^n \frac{\partial f}{\partial x_k} (\ba) (x_k - a_k) \\
   \nabla f(\ba)^T (\bx - \ba)
   &amp;= \frac{\partial f}{\partial x_i} (\ba) (x_i - a_i)
   + \frac{\partial f}{\partial x_j} (\ba) (x_j - a_j) \\
   &amp;= - \frac{\partial f}{\partial x_i} (\ba) + \frac{\partial f}{\partial x_j} (\ba)\\
   &lt; 0.
   \end{split}\]</div>
</li>
<li><p>This violates the hypothesis that (I) holds true.</p></li>
<li><p>Thus, (II) must be true.</p></li>
<li><p>Thus, (I) implies (II).</p></li>
</ol>
</div>
</div></div>
<div class="section" id="unit-ball-constraint">
<h3><span class="section-number">9.5.2.5. </span>Unit Ball Constraint<a class="headerlink" href="#unit-ball-constraint" title="Permalink to this headline">Â¶</a></h3>
<div class="proof example admonition" id="ex-cvxopt-diff-obj-unit-ball">
<p class="admonition-title"><span class="caption-number">Example 9.9 </span> (Minimization over unit ball)</p>
<div class="example-content section" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(f : \RR^n \to \RR\)</span> be a convex function
which is differentiable over <span class="math notranslate nohighlight">\(B[\bzero, 1]\)</span>.
Consider the minimization problem:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
&amp; \text{minimize }  &amp;  &amp; f(\bx) \\
&amp; \text{subject to } &amp; &amp; \| \bx \|  \leq 1.
\end{split}\]</div>
<ol>
<li><p>The feasible set is given by</p>
<div class="math notranslate nohighlight">
\[
   C = B[\bzero, 1] = \{  \bx \ST \| \bx \| \leq 1 \}.
   \]</div>
</li>
<li><p>A point <span class="math notranslate nohighlight">\(\ba \in B[\bzero, 1]\)</span> is an optimal point
if and only if</p>
<div class="math notranslate nohighlight">
\[
   \nabla f(\ba)^T (\bx - \ba) \geq 0 \Forall \bx \in B[\bzero, 1].
   \]</div>
</li>
<li><p>This is equivalent to saying that</p>
<div class="math notranslate nohighlight">
\[
   \underset{\| \bx \| \leq 1}{\inf} (\nabla f(\ba)^T \bx - \nabla f(\ba)^T \ba ) \geq 0.
   \]</div>
</li>
<li><p>Recall from <a class="reference internal" href="../num_opt/opt_intro.html#res-opt-min-linear-func-unit-ball">Theorem 7.11</a>
that for any <span class="math notranslate nohighlight">\(\bv \in \RR^n\)</span>, the optimal value of the problem</p>
<div class="math notranslate nohighlight">
\[
   \inf \{ \bv^T \bx  \ST  \| \bx \| \leq 1 \}
   \]</div>
<p>is <span class="math notranslate nohighlight">\(-\| \bv \|\)</span>.</p>
</li>
<li><p>Thus,</p>
<div class="math notranslate nohighlight">
\[
   \underset{\| \bx \| \leq 1}{\inf} \nabla f(\ba)^T \bx = - \| \nabla f(\ba) \|.
   \]</div>
</li>
<li><p>Thus, the inequality simplifies to</p>
<div class="math notranslate nohighlight">
\[
   - \nabla f(\ba)^T \ba \geq  \| \nabla f(\ba) \|.
   \]</div>
</li>
<li><p>At the same time, by Cauchy Schwartz inequality,</p>
<div class="math notranslate nohighlight">
\[
   - \nabla f(\ba)^T \ba \leq  \| \nabla f(\ba) \| \| \ba \|
   \leq \| \nabla f(\ba) \|
   \]</div>
<p>since <span class="math notranslate nohighlight">\(\ba \in B[\bzero, 1]\)</span>.</p>
</li>
<li><p>Thus, the inequality must be an equality, giving us,
<span class="math notranslate nohighlight">\(\ba\)</span> is an optimal point</p>
<div class="math notranslate nohighlight">
\[
   - \nabla f(\ba)^T \ba =  \| \nabla f(\ba) \|.
   \]</div>
</li>
</ol>
<p>We now have following possibilities for this condition.</p>
<ol>
<li><p>If <span class="math notranslate nohighlight">\(\nabla f(\ba) = \bzero\)</span>, then the condition holds
and <span class="math notranslate nohighlight">\(\ba\)</span> is indeed an optimal point.</p></li>
<li><p>Otherwise, if <span class="math notranslate nohighlight">\(\nabla f(\ba) \neq \bzero\)</span>, then <span class="math notranslate nohighlight">\(\| \ba \| = 1\)</span> must be true.</p>
<ol>
<li><p>For contradiction, if we assume that <span class="math notranslate nohighlight">\( \| \ba \| &lt; 1\)</span>.</p></li>
<li><p>Then, by Cauchy Schwartz inequality</p>
<div class="math notranslate nohighlight">
\[
      - \nabla f(\ba)^T \ba \leq  \| \nabla f(\ba) \| \| \ba \|
      &lt; \| \nabla f(\ba) \|,
      \]</div>
<p>a contradiction.</p>
</li>
</ol>
</li>
<li><p>Thus, if <span class="math notranslate nohighlight">\(\nabla f(\ba) \neq \bzero\)</span>, then <span class="math notranslate nohighlight">\(\ba\)</span> is an optimal point
if and only if <span class="math notranslate nohighlight">\( \| \ba \| = 1\)</span> and</p>
<div class="math notranslate nohighlight">
\[
   - \nabla f(\ba)^T \ba =  \| \nabla f(\ba) \| = \| \nabla f(\ba) \| \| \ba \|.
   \]</div>
</li>
<li><p>But this is possible only when there exists <span class="math notranslate nohighlight">\(t \leq 0\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
   \nabla f(\ba) = t \ba.
   \]</div>
</li>
<li><p>Thus, if <span class="math notranslate nohighlight">\(\nabla f(\ba) \neq \bzero\)</span>, then <span class="math notranslate nohighlight">\(\ba\)</span> is an optimal point
if and only if <span class="math notranslate nohighlight">\( \| \ba \| = 1\)</span> and there exists <span class="math notranslate nohighlight">\(t \leq 0\)</span> such that
<span class="math notranslate nohighlight">\(\nabla f(\ba) = t \ba\)</span>.</p></li>
</ol>
</div>
</div></div>
</div>
<div class="section" id="descent-directions-method">
<h2><span class="section-number">9.5.3. </span>Descent Directions Method<a class="headerlink" href="#descent-directions-method" title="Permalink to this headline">Â¶</a></h2>
<p>We first consider the problem of unconstrained minimization of
a continuously differentiable function <span class="math notranslate nohighlight">\(f\)</span>.</p>
<p>Typical iterative algorithms which aim to find
the solution <span class="math notranslate nohighlight">\(\bx\)</span> for the minimization problem
start with an initial guess <span class="math notranslate nohighlight">\(\bx_0\)</span> and perform
a step of the form</p>
<div class="math notranslate nohighlight">
\[
\bx_{k+1} = \bx_k + t_k \bd_k
\]</div>
<p>where <span class="math notranslate nohighlight">\(\bx_k\)</span> is the current guess (starting from <span class="math notranslate nohighlight">\(\bx_0\)</span>),
<span class="math notranslate nohighlight">\(\bd_k\)</span> is a direction in which we move to make the next
guess and <span class="math notranslate nohighlight">\(t_k\)</span> is a step size in that direction.
<span class="math notranslate nohighlight">\(\bx_{k+1}\)</span> is the next guess obtained from current
guess.
We say that an algorithm has made progress if
<span class="math notranslate nohighlight">\(f(\bx_{k+1}) &lt; f(\bx_k)\)</span>.</p>
<p>This brings us to the notion of a descent direction.</p>
<div class="proof definition admonition" id="def-opt-descent-direction">
<span id="index-1"></span><p class="admonition-title"><span class="caption-number">Definition 9.22 </span> (Descent direction)</p>
<div class="definition-content section" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(f : \VV \to \RR\)</span> be a continuously differentiable
function over <span class="math notranslate nohighlight">\(\VV\)</span>. A nonzero vector <span class="math notranslate nohighlight">\(\bd\)</span> is called
a descent direction of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(\bx\)</span> if the
directional derivative <span class="math notranslate nohighlight">\(f'(\bx; \bd)\)</span> is negative.</p>
<p>In other words,</p>
<div class="math notranslate nohighlight">
\[
f'(\bx; \bd) = \langle \bd, \nabla f(\bx) \rangle &lt; 0.
\]</div>
</div>
</div><p>If the directional derivative is negative, then it
is clear that for a small enough step in this direction,
the value of <span class="math notranslate nohighlight">\(f\)</span> will decrease.</p>
<div class="proof lemma admonition" id="res-opt-descent-dir-property">
<p class="admonition-title"><span class="caption-number">Lemma 9.4 </span> (Descent property of descent direction)</p>
<div class="lemma-content section" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(f : \VV \to \RR\)</span> be a continuously differentiable
function over <span class="math notranslate nohighlight">\(\VV\)</span>.
Let <span class="math notranslate nohighlight">\(\bx \in \VV\)</span>.
Assume that <span class="math notranslate nohighlight">\(\bd\)</span> is a descent direction for <span class="math notranslate nohighlight">\(f\)</span>.
Then, there exists <span class="math notranslate nohighlight">\(\epsilon &gt; 0\)</span>  such that</p>
<div class="math notranslate nohighlight">
\[
f(\bx + t \bd) &lt; f(\bx)
\]</div>
<p>for any <span class="math notranslate nohighlight">\(t \in (0, \epsilon]\)</span>.</p>
</div>
</div><div class="proof admonition" id="proof">
<p>Proof. This follows from the negativity of the directional derivative.</p>
<ol>
<li><p>Recall from <a class="reference internal" href="../convex_sets/directional_derivatives.html#def-cvxf-directional-derivative">Definition 8.70</a> that</p>
<div class="math notranslate nohighlight">
\[
   f'(\bx; \bd) = \lim_{t \to 0^+} \frac{f(\bx + t \bd) - f(\bx)}{t}.
   \]</div>
</li>
<li><p>Since <span class="math notranslate nohighlight">\(\bd\)</span> is a descent direction, hence <span class="math notranslate nohighlight">\(f'(\bx; \bd) &lt; 0\)</span>.</p></li>
<li><p>Thus,</p>
<div class="math notranslate nohighlight">
\[
   \lim_{t \to 0^+} \frac{f(\bx + t \bd) - f(\bx)}{t} &lt; 0.
   \]</div>
</li>
<li><p>Thus, there exists <span class="math notranslate nohighlight">\(\epsilon &gt; 0\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
   \frac{f(\bx + t \bd) - f(\bx)}{t} &lt; 0
   \]</div>
<p>for every <span class="math notranslate nohighlight">\(t \in (0, \epsilon]\)</span>.</p>
</li>
</ol>
</div>
<div class="section" id="id2">
<h3><span class="section-number">9.5.3.1. </span>Descent Directions Method<a class="headerlink" href="#id2" title="Permalink to this headline">Â¶</a></h3>
<div class="proof algorithm admonition" id="alg-opt-diff-obj-descent-directions">
<p class="admonition-title"><span class="caption-number">Algorithm 9.1 </span> (The descent directions method)</p>
<div class="algorithm-content section" id="proof-content">
<p>Initialization</p>
<ol class="simple">
<li><p>Pick <span class="math notranslate nohighlight">\(\bx_0 \in \VV\)</span> arbitrarily as initial solution.</p></li>
</ol>
<p>General iteration: for <span class="math notranslate nohighlight">\(k=0,1,2,\dots\)</span>, execute the following steps</p>
<ol class="simple">
<li><p>Pick a descent direction <span class="math notranslate nohighlight">\(\bd_k\)</span>.</p></li>
<li><p>Pick a step size <span class="math notranslate nohighlight">\(t_k\)</span> satisfying <span class="math notranslate nohighlight">\(f(\bx + t_k \bd_k) &lt; f(\bx_k)\)</span>.</p></li>
<li><p>Update: <span class="math notranslate nohighlight">\(\bx_{k+1} \leftarrow \bx_k + t_k \bd_k\)</span>.</p></li>
<li><p>Check for convergence: If converged, then STOP.</p></li>
</ol>
<p>Return <span class="math notranslate nohighlight">\(\bx_{k+1}\)</span> as the output.</p>
</div>
</div><p>This method is not really an actual algorithm.
It can be considered as a template for an actual
algorithm. Several aspects of the method need
to be carefully chosen to come up with a viable
algorithm.</p>
<ol class="simple">
<li><p>How to select the initial point <span class="math notranslate nohighlight">\(\bx_0\)</span>?</p></li>
<li><p>How to choose the descent direction?</p></li>
<li><p>How to select the step size?</p></li>
<li><p>How to decide when to stop the iterations (stopping criterion)?</p></li>
</ol>
<p>The key result that we establish here is to show that
if the step size <span class="math notranslate nohighlight">\(t_k\)</span> is sufficient small
in the descent direction <span class="math notranslate nohighlight">\(\bd_k\)</span>, then there is sufficient
decrease in the value of <span class="math notranslate nohighlight">\(f\)</span> going from <span class="math notranslate nohighlight">\(\bx_k\)</span> to <span class="math notranslate nohighlight">\(\bx_{k+1}\)</span>.</p>
<div class="proof theorem admonition" id="res-opt-diff-obj-descent-dir-suff-dec">
<p class="admonition-title"><span class="caption-number">Theorem 9.52 </span> (Sufficient decrease condition for descent direction method)</p>
<div class="theorem-content section" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(f\)</span> be a continuously differentiable function over <span class="math notranslate nohighlight">\(\RR^n\)</span>.
Let <span class="math notranslate nohighlight">\(\bx \in \RR^n\)</span>.
Assume that a nonzero <span class="math notranslate nohighlight">\(\bd\)</span> is a descent direction for <span class="math notranslate nohighlight">\(f\)</span>
at <span class="math notranslate nohighlight">\(\bx\)</span>.
Let <span class="math notranslate nohighlight">\(\alpha \in (0, 1)\)</span>.
Then, there exists <span class="math notranslate nohighlight">\(\epsilon &gt; 0\)</span> such that the inequality</p>
<div class="math notranslate nohighlight">
\[
f(\bx) - f(\bx + t \bd) \geq - \alpha t \langle \bd, \nabla f(\bx) \rangle
\]</div>
<p>holds for every <span class="math notranslate nohighlight">\(t \in [0, \epsilon]\)</span>.</p>
</div>
</div><div class="proof admonition" id="proof">
<p>Proof. We proceed as follows.</p>
<ol>
<li><p>Since <span class="math notranslate nohighlight">\(f\)</span> is continuously differentiable,
hence due to <a class="reference internal" href="../mv_calculus/differentiation.html#res-mvc-first-order-approx">Theorem 5.1</a>,</p>
<div class="math notranslate nohighlight">
\[
   f(\bx + t \bd) = f(\bx) + t \nabla f(\bx)^T \bd  + o(t \| \bd \|).
   \]</div>
</li>
<li><p>Rearranging the terms and introducing an <span class="math notranslate nohighlight">\(\alpha \in (0, 1)\)</span>, we obtain</p>
<div class="math notranslate nohighlight">
\[
   f(\bx) - f(\bx + t \bd) = - \alpha t \nabla f(\bx)^T \bd 
   - (1- \alpha) \nabla f(\bx)^T \bd - o(t \| \bd \|).
   \]</div>
</li>
<li><p>Since <span class="math notranslate nohighlight">\(\bd\)</span> is a descent direction of <span class="math notranslate nohighlight">\(f\)</span>, hence <span class="math notranslate nohighlight">\(f(\bx; \bd) = \nabla f(\bx)^T \bd &lt; 0\)</span>.</p></li>
<li><p>Thus,</p>
<div class="math notranslate nohighlight">
\[
   \lim_{t \to 0^+} \frac{(1- \alpha) \nabla f(\bx)^T \bd + o(t \| \bd \|)}{t}
   = (1- \alpha) \nabla f(\bx)^T \bd  &lt; 0.
   \]</div>
</li>
<li><p>Hence, there exists <span class="math notranslate nohighlight">\(\epsilon &gt; 0\)</span> such that for every <span class="math notranslate nohighlight">\(t \in [0, \epsilon]\)</span>,</p>
<div class="math notranslate nohighlight">
\[
   (1- \alpha) \nabla f(\bx)^T \bd + o(t \| \bd \| &lt; 0.
   \]</div>
</li>
<li><p>Thus, from the previous equation:</p>
<div class="math notranslate nohighlight">
\[
   f(\bx) - f(\bx + t \bd) \geq - \alpha t \nabla f(\bx)^T \bd
   \]</div>
<p>for every <span class="math notranslate nohighlight">\(t \in [0, \epsilon]\)</span>.</p>
</li>
</ol>
</div>
</div>
<div class="section" id="step-size-selection">
<h3><span class="section-number">9.5.3.2. </span>Step Size Selection<a class="headerlink" href="#step-size-selection" title="Permalink to this headline">Â¶</a></h3>
<p>Following are common methods for step size selection.
Each method has has advantages as well as drawbacks.</p>
<ol class="simple">
<li><p>Constant step size</p></li>
<li><p>Exact line search</p></li>
<li><p>Backtracking</p></li>
</ol>
<p>Constant step size uses <span class="math notranslate nohighlight">\(t_k = \bar{t}\)</span> for every iteration.</p>
<ul class="simple">
<li><p>A large step size might cause algorithm to take non-decreasing steps.</p></li>
<li><p>The algorithm may never converge with a large step size.</p></li>
<li><p>A small constant step size may lead to very slow convergence.</p></li>
</ul>
<p>Exact line search solves the minimization problem</p>
<div class="math notranslate nohighlight">
\[
\text{ minimize } f \text{ along the ray } \bx_k + t \bd_k.
\]</div>
<p>The optimization variable is the step size parameter <span class="math notranslate nohighlight">\(t \in \RR_+\)</span>.
The solution is the value <span class="math notranslate nohighlight">\(t_k \geq 0\)</span>.</p>
<ul class="simple">
<li><p>Minimizing <span class="math notranslate nohighlight">\(f\)</span> along the ray may not be straight-forward.</p></li>
<li><p>Any closed form or algorithmic solution for the exact line search may not be available.</p></li>
</ul>
<p>Backtracking is a compromise between these two approaches.</p>
<ol>
<li><p>Input parameters <span class="math notranslate nohighlight">\(s &gt; 0\)</span>, <span class="math notranslate nohighlight">\(\alpha \in (0,1)\)</span>, <span class="math notranslate nohighlight">\(\beta \in (0, 1)\)</span>.</p></li>
<li><p>Initialize <span class="math notranslate nohighlight">\(t_k = s\)</span>.</p></li>
<li><p>While</p>
<div class="math notranslate nohighlight">
\[
   f(\bx_k) - f(\bx_k + t_k \bd_k) &lt; - \alpha t_k \nabla f(\bx_k)^T \bd
   \]</div>
<ol class="simple">
<li><p>Set <span class="math notranslate nohighlight">\(t_k \leftarrow \beta t_k\)</span>.</p></li>
<li><p>Continue.</p></li>
</ol>
</li>
<li><p>Return step size <span class="math notranslate nohighlight">\(t_k\)</span>.</p></li>
</ol>
<p>Essentially, we are reducing the step  size by a factor <span class="math notranslate nohighlight">\(\beta\)</span> iteratively
till <span class="math notranslate nohighlight">\(f\)</span> shows sufficient decrease as stipulated by
<a class="reference internal" href="#res-opt-diff-obj-descent-dir-suff-dec">Theorem 9.52</a>.</p>
<ul class="simple">
<li><p>There is no exact line search.</p></li>
<li><p>Does find a good enough step size satisfying the sufficient decrease condition.</p></li>
<li><p>Involves multiple evaluations of <span class="math notranslate nohighlight">\(f\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(s\)</span> should be chosen carefully.</p></li>
</ul>
</div>
</div>
<div class="section" id="gradient-method">
<h2><span class="section-number">9.5.4. </span>Gradient Method<a class="headerlink" href="#gradient-method" title="Permalink to this headline">Â¶</a></h2>
<p>The gradient method is a descent direction method in which
the descent direction is always chosen to be the negative
of the gradient at the current point (solution).</p>
<div class="math notranslate nohighlight">
\[
\bd_k  = - \nabla f(\bx_k).
\]</div>
<div class="proof lemma admonition" id="res-opt-neg-grad-decent-dir">
<p class="admonition-title"><span class="caption-number">Lemma 9.5 </span> (Negative of gradient is a descent direction)</p>
<div class="lemma-content section" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(f\)</span> be a continuously differentiable function over an open set <span class="math notranslate nohighlight">\(S\)</span>.
At any point <span class="math notranslate nohighlight">\(\bx \in S\)</span>, the negative of the gradient <span class="math notranslate nohighlight">\(\bd = - \nabla f(\bx)\)</span>
is a descent direction whenever <span class="math notranslate nohighlight">\(\nabla f(\bx) \neq \bzero\)</span>.</p>
</div>
</div><div class="proof admonition" id="proof">
<p>Proof. We just need to compute the directional derivative.</p>
<div class="math notranslate nohighlight">
\[
f'(\bx; \bd) = \langle \bd, \nabla f(\bx) \rangle
= \langle -\nabla f(\bx), \nabla f(\bx) \rangle
= - \| \nabla f(\bx) \|^2 &lt; 0.
\]</div>
<p>The last strict inequality is valid since <span class="math notranslate nohighlight">\(\nabla f(\bx) \neq \bzero\)</span>.</p>
</div>
</div>
<div class="section" id="gradient-projection-method">
<h2><span class="section-number">9.5.5. </span>Gradient Projection Method<a class="headerlink" href="#gradient-projection-method" title="Permalink to this headline">Â¶</a></h2>
<p>In this subsection, we present a method to solve the
optimization problem</p>
<div class="math notranslate nohighlight">
\[\begin{split}
&amp; \text{minimize }  &amp;  &amp; f(\bx) \\
&amp; \text{subject to } &amp; &amp; \bx \in C
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(C\)</span> is a convex set and <span class="math notranslate nohighlight">\(f\)</span> is differentiable over <span class="math notranslate nohighlight">\(C\)</span>.</p>
<p>Recall from <a class="reference internal" href="#res-opt-over-c-stationary-orth-proj">Theorem 9.46</a>, that
<span class="math notranslate nohighlight">\(\ba\)</span> is a stationary point for the problem of minimizing
<span class="math notranslate nohighlight">\(f\)</span> over a convex set <span class="math notranslate nohighlight">\(C\)</span> if and only if</p>
<div class="math notranslate nohighlight">
\[
\ba = P_C (\ba - s \nabla f(\ba)).
\]</div>
<p>This stationarity condition is the basis for the gradient projection method
presented below.</p>
<div class="proof algorithm admonition" id="alg-opt-diff-obj-gradient-projection">
<p class="admonition-title"><span class="caption-number">Algorithm 9.2 </span> (The gradient projection method)</p>
<div class="algorithm-content section" id="proof-content">
<p>Inputs</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(\epsilon &gt; 0\)</span> - tolerance parameter</p></li>
</ol>
<p>Initialization</p>
<ol class="simple">
<li><p>Pick <span class="math notranslate nohighlight">\(\bx_0 \in C\)</span> arbitrarily.</p></li>
</ol>
<p>General iteration: for <span class="math notranslate nohighlight">\(k=0,1,2,\dots\)</span>, execute the following steps</p>
<ol class="simple">
<li><p>Pick a step size <span class="math notranslate nohighlight">\(t_k\)</span> by a line search procedure.</p></li>
<li><p>Update: <span class="math notranslate nohighlight">\(\bx_{k+1} \leftarrow P_C (\bx_k - t_k \nabla f(\bx_k))\)</span>.</p></li>
<li><p>Check for convergence: If <span class="math notranslate nohighlight">\(\| \bx_{k+1} - \bx_k \| \leq \epsilon\)</span>, then
STOP.</p></li>
</ol>
<p>Return <span class="math notranslate nohighlight">\(\bx_{k+1}\)</span> as the output.</p>
</div>
</div><div class="docutils">
<p>In the case of unconstrained minimization:</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(C = \RR^n\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P_C (\bx_k - t_k \nabla f(\bx_k)) = \bx_k - t_k \nabla f(\bx_k)\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\bx_{k+1} = \bx_k - t_k \nabla f(\bx_k)\)</span>.</p></li>
<li><p>We see that gradient projection reduces to gradient descent.</p></li>
</ol>
<p>Another way to look at the algorithm is:</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(\by_{k+1} = \bx_k - t_k \nabla f(\bx_k)\)</span> computes the
next candidate solution assuming no constraints.</p></li>
<li><p><span class="math notranslate nohighlight">\(\bx_{k+1} = P_C(\by_{k+1})\)</span> step projects the next candidate
solution back to the feasible set <span class="math notranslate nohighlight">\(C\)</span>.</p></li>
<li><p>Thus, we have a gradient step followed by a projection step.</p></li>
</ol>
</div>
<div class="section" id="convergence">
<h3><span class="section-number">9.5.5.1. </span>Convergence<a class="headerlink" href="#convergence" title="Permalink to this headline">Â¶</a></h3>
<p>If we can establish conditions under which each iteration of
the gradient projection algorithm leads to sufficient decrease
in the value of the objective function, then we can guarantee
that the algorithm will converge in a finite number of steps.</p>
<div class="proof lemma admonition" id="res-opt-grad-proj-suff-dec">
<p class="admonition-title"><span class="caption-number">Lemma 9.6 </span> (Sufficient decrease lemma for constrained problems)</p>
<div class="lemma-content section" id="proof-content">
<p>Consider the optimization problem:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
&amp; \text{minimize }  &amp;  &amp; f(\bx) \\
&amp; \text{subject to } &amp; &amp; \bx \in C.
\end{split}\]</div>
<p>Assume that <span class="math notranslate nohighlight">\(C\)</span> is a nonempty closed convex set,
<span class="math notranslate nohighlight">\(f\)</span> is continuously differentiable over <span class="math notranslate nohighlight">\(C\)</span>, and <span class="math notranslate nohighlight">\(\nabla f\)</span>
is Lipschitz continuous with a constant <span class="math notranslate nohighlight">\(L\)</span> over <span class="math notranslate nohighlight">\(C\)</span>,
Then, for any <span class="math notranslate nohighlight">\(\bx \in C\)</span>, and <span class="math notranslate nohighlight">\(t \in (0, \frac{2}{L})\)</span>,
the following inequality holds.</p>
<div class="math notranslate nohighlight">
\[
f(\bx) - f(\by) \geq t \left ( 1  - \frac{L t}{2} \right )
\left \| \frac{1}{t}(\bx - \by)  \right \|^2.
\]</div>
<p>where <span class="math notranslate nohighlight">\(\by = P_C(\bx - t \nabla f(\bx))\)</span>.</p>
</div>
</div><div class="proof admonition" id="proof">
<p>Proof.</p>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./cvxopt"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="duality.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">9.4. </span>Basic Duality</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="linear_constraints.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">9.6. </span>Linear Constraints</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Shailesh Kumar<br/>
    
        &copy; Copyright 2021-2022.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>